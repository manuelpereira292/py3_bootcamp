{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 13: Exercise solutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../course/common.py') as fin:\n",
    "    exec(fin.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../course/matplotlibconf.py') as fin:\n",
    "    exec(fin.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's deploy an image recognition API using Tensorflow Serving. The main difference from the API we have deployed in this lab is that we will have to deal with how to pass an image to the model through tensorflow serving. Since this lab focuses on deployment, we will take a shortcut and deploy a pre-trained model that uses Imagenet. In particular, we will deploy the `Xception` model. If you are unsure about how to use a pre-trained model, please go back to [Lab 11](./11_Pretrained_models_for_images.ipynb) for a refresher.\n",
    "\n",
    "Here are the steps you will need to complete:\n",
    "\n",
    "- load the model in Keras\n",
    "- export the model for tensorflow serving:\n",
    "    - set the learning phase to zero\n",
    "    - save the model with `tf.saved_model.save`\n",
    "- run the model server\n",
    "- write a short script that:\n",
    "    - loads an image\n",
    "    - pre-processes it with the appropriate function\n",
    "    - serializes the image to Protobuf\n",
    "    - sends the image to the server\n",
    "    - receives a prediction\n",
    "    - decodes the prediction with Keras `decode_prediction` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from os.path import join\n",
    "import shutil\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.applications.xception import Xception\n",
    "from tensorflow.keras.applications.xception import preprocess_input\n",
    "from tensorflow.keras.applications.xception import decode_predictions\n",
    "\n",
    "from grpc import insecure_channel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow_serving.apis.prediction_service_pb2_grpc \\\n",
    "    import PredictionServiceStub\n",
    "from tensorflow_serving.apis.predict_pb2 \\\n",
    "    import PredictRequest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save Xception as tensorflow model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Xception(weights='imagenet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.set_learning_phase(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = '/tmp/ztdl_models/xception'\n",
    "sub_path = 'tfserving'\n",
    "version = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "export_path = join(base_path, sub_path, str(version))\n",
    "shutil.rmtree(export_path, ignore_errors=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.saved_model.save(model, export_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "MetaGraphDef with tag-set: 'serve' contains the following SignatureDefs:\r\n",
      "\r\n",
      "signature_def['__saved_model_init_op']:\r\n",
      "  The given SavedModel SignatureDef contains the following input(s):\r\n",
      "  The given SavedModel SignatureDef contains the following output(s):\r\n",
      "    outputs['__saved_model_init_op'] tensor_info:\r\n",
      "        dtype: DT_INVALID\r\n",
      "        shape: unknown_rank\r\n",
      "        name: NoOp\r\n",
      "  Method name is: \r\n",
      "\r\n",
      "signature_def['serving_default']:\r\n",
      "  The given SavedModel SignatureDef contains the following input(s):\r\n",
      "    inputs['input_1'] tensor_info:\r\n",
      "        dtype: DT_FLOAT\r\n",
      "        shape: (-1, 299, 299, 3)\r\n",
      "        name: serving_default_input_1:0\r\n",
      "  The given SavedModel SignatureDef contains the following output(s):\r\n",
      "    outputs['predictions'] tensor_info:\r\n",
      "        dtype: DT_FLOAT\r\n",
      "        shape: (-1, 1000)\r\n",
      "        name: StatefulPartitionedCall:0\r\n",
      "  Method name is: tensorflow/serving/predict\r\n"
     ]
    }
   ],
   "source": [
    "!saved_model_cli show --dir {export_path} --all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Start Server"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "docker run \\\n",
    "    -v /tmp/ztdl_models/xception/tfserving/:/models/xception \\\n",
    "    -e MODEL_NAME=xception \\\n",
    "    -e MODEL_PATH=/models/xception \\\n",
    "    -p 8502:8500  \\\n",
    "    -p 8503:8501  \\\n",
    "    -t tensorflow/serving\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert image to protobuf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = image.load_img(\n",
    "    './13_penguin.jpg', target_size=(299, 299))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_tensor = np.expand_dims(\n",
    "    image.img_to_array(img), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_scaled = preprocess_input(img_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_pb = tf.compat.v1.make_tensor_proto(\n",
    "    img_scaled, dtype='float', shape=img_scaled.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Send request and retrieve response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "channel = insecure_channel('localhost:8502')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "stub = PredictionServiceStub(channel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "request = PredictRequest()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "request.model_spec.name = 'xception'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "request.model_spec.signature_name = 'serving_default'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "request.inputs['input_1'].CopyFrom(data_pb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_future = stub.Predict.future(request, 5.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = result_future.result()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decode predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = tf.make_ndarray(result.outputs['predictions'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = decode_predictions(scores, top=1)[0][0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'king_penguin'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above method of serving a pre-trained model has an issue: we are doing pre-processing and prediction decoding on the client side. This is not a best practice, because it requires the client to be aware of what kind of pre-processing and decoding functions the model needs.\n",
    "\n",
    "We want a server that takes the image as it is and returns a string with the name of the object found. \n",
    "\n",
    "The easy way to do this is to use the Flask app implementation we have shown in this lab and move pre-processing and decoding on the server side.\n",
    "\n",
    "Go ahead and build a Flask version of the API that takes an image URL as a JSON string, applies pre-processing, runs and decodes the prediction and returns a string with the response.\n",
    "\n",
    "You will not use tensorflow serving for this exercise.\n",
    "\n",
    "Once your script is ready, save it as `13_flask_serve_xception.py`, run it as:\n",
    "\n",
    "```\n",
    "python 13_flask_serve_xception.py\n",
    "```\n",
    "\n",
    "and test the prediction with the following command:\n",
    "\n",
    "    curl -d \"http://bit.ly/2wb7uqN\" \\\n",
    "         -H \"Content-Type: application/json\" \\\n",
    "         -X POST http://localhost:5000\n",
    "\n",
    "If you've done things correctly, this should return:\n",
    "\n",
    "    \"king_penguin\"\n",
    "\n",
    "**Disclaimer: this script is not for production purposes. Retrieving a file from a URL is not secure, and you should avoid building an API that retrieves a file from a URL provided from the client. Here we used the URL retrieval trick to make the curl command shorter.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import os\r\n",
      "import json\r\n",
      "import numpy as np\r\n",
      "\r\n",
      "from flask import Flask\r\n",
      "from flask import request, jsonify\r\n",
      "\r\n",
      "import tensorflow as tf\r\n",
      "from urllib.request import urlretrieve\r\n",
      "from tensorflow.keras.preprocessing import image\r\n",
      "from tensorflow.keras.applications.xception import Xception\r\n",
      "from tensorflow.keras.applications.xception import preprocess_input\r\n",
      "from tensorflow.keras.applications.xception import decode_predictions\r\n",
      "\r\n",
      "loaded_model = None\r\n",
      "\r\n",
      "app = Flask(__name__)\r\n",
      "\r\n",
      "\r\n",
      "def load_model():\r\n",
      "    \"\"\"\r\n",
      "    Load model and tensorflow graph\r\n",
      "    into global variables.\r\n",
      "    \"\"\"\r\n",
      "\r\n",
      "    # global variables\r\n",
      "    global loaded_model\r\n",
      "\r\n",
      "    loaded_model = Xception(weights='imagenet')\r\n",
      "    print(\"Model loaded.\")\r\n",
      "\r\n",
      "def load_image_from_url(url, target_size=(299, 299)):\r\n",
      "    path, response = urlretrieve(url, filename='/tmp/temp_img')\r\n",
      "    img = image.load_img(path, target_size=target_size)\r\n",
      "    img_tensor = np.expand_dims(image.img_to_array(img), axis=0)\r\n",
      "    return img, img_tensor\r\n",
      "\r\n",
      "def preprocess(data):\r\n",
      "    url = data.decode('utf-8')\r\n",
      "    img, img_tensor = load_image_from_url(url)\r\n",
      "    img_scaled = preprocess_input(img_tensor)\r\n",
      "    return img_scaled\r\n",
      "\r\n",
      "\r\n",
      "@app.route('/', methods=[\"POST\"])\r\n",
      "def predict():\r\n",
      "    \"\"\"\r\n",
      "    Generate predictions with the model\r\n",
      "    when receiving data as a POST request\r\n",
      "    \"\"\"\r\n",
      "    if request.method == \"POST\":\r\n",
      "        # get url from the request\r\n",
      "        data = request.data\r\n",
      "\r\n",
      "        # preprocess the data\r\n",
      "        processed = preprocess(data)\r\n",
      "\r\n",
      "        # run predictions\r\n",
      "        preds = loaded_model.predict(processed)\r\n",
      "\r\n",
      "        # obtain predicted classes from predicted probabilities\r\n",
      "        result = decode_predictions(preds, top=1)[0][0][1]\r\n",
      "\r\n",
      "        # print in backend\r\n",
      "        print(\"Received data:\", data)\r\n",
      "        print(\"Predicted labels:\", result)\r\n",
      "\r\n",
      "        return jsonify(result)\r\n",
      "\r\n",
      "\r\n",
      "if __name__ == \"__main__\":\r\n",
      "    print(\"* Loading model and starting Flask server...\")\r\n",
      "    load_model()\r\n",
      "    app.run(host='0.0.0.0', debug=True)\r\n",
      "\r\n",
      "\r\n",
      "# Test this with the following command:\r\n",
      "# curl -d 'http://bit.ly/2wb7uqN' -H \"Content-Type: application/json\" -X POST http://localhost:5000\r\n"
     ]
    }
   ],
   "source": [
    "!cat 13_flask_serve_xception.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "title": "Serving Deep Learning Models Exercises Solutions"
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
