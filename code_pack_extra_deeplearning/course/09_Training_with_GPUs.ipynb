{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 9: Training with GPUs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab, we will learn how to leverage Graphical Processing Units (GPUs) to speed up the training of our models. The faster a model trains, the more experiments we can run, and therefore the better solutions we can find. Also, leveraging cloud GPUs has become so easy by now that it would be a pity not to take advantage of this opportunity. Only a few years ago, training a deep Neural Network using a GPU was a skill that demanded very sophisticated knowledge and much money. Nowadays, we train a model on many GPUs at a relatively affordable cost.\n",
    "\n",
    "We will start this lab by introducing what a GPU is, where it can be found, what kinds of GPUs are available and why they are so useful to do Deep Learning. Then we will review several cloud providers of GPUs and guide you through how to use them. Once we have a working cloud instance with one or more GPUs, we will compare training a model with and without a GPU, and appreciate the speedup, especially with Convolutional Neural Networks. We will then extend training to multiple GPUs and introduce a few ways to use multiple GPUs in Keras.\n",
    "\n",
    "This lab is a bit different from the other labs as there will be less Python code and more links to external documentation and services. Also, while we will do our best to have the most up to date guide to currently existing providers, it is important that you understand how fast the landscape is evolving. During the course of the past six months, each of the providers presented introduced newer and easier ways to access cloud GPUs, making the previous documentation obsolete. Thus, it is important that you understand the principles of why accelerated hardware helps and when. If you do this, it will be easy to adapt to new ways of doing things when they come out. All that said, let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graphical Processing Units"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Graphical Processing Units](https://en.wikipedia.org/wiki/Graphics_processing_unit) are computer chips that specialize in the parallel manipulation of huge, multi-dimensional arrays. Originally developed to accelerate the display of video games graphics, they are today widely used for other purposes like Machine Learning acceleration.\n",
    "\n",
    "The term GPU became famous in 1999 when Nvidia - the dominant player in the field today - marketed the _GeForce 256_ as \"the world's first GPU\". In 2002, ATI Technologies, a competitor of Nvidia, coined the term \"visual processing unit\" or VPU with the release of the _Radeon 9700_. The following picture shows the original _GeForce 256_ (left side) and the _GeForce GTX 1080_ (right), one of the latest released and most powerful graphics cards in the market.\n",
    "\n",
    "![NVIDIA Graphics cards](./assets/Nvidia_GPU.jpg)\n",
    "\n",
    "In 2006, Nvidia came out with a high-level language called [CUDA](https://en.wikipedia.org/wiki/CUDA) (Compute Unified Device Architecture), that helps software developers and engineers to write programs from graphics processors in a high-level language – an approach termed GPGPU (General-Purpose computing on Graphics Processing Units). CUDA is a language that gives direct access to the GPU's virtual instruction set and parallel computational elements, for the execution of compute kernels. This was probably one of the most significant changes in the way researchers and developers interacted with GPUs.\n",
    "\n",
    "Why are GPUs, initially developed for video games graphics, so useful for Deep Learning?\n",
    "\n",
    "As you already know, training a Neural Network requires several operations, many of which involve large matrix multiplications. They perform matrix multiplications in the forward pass when inputs (or activations) and weights are multiplied (see [Lab 5](./05_Deep_Learning_Internals.ipynb) if you need a refresher on the math). Back-propagation also involves matrix multiplications, when the error is propagated back through the network to adjust the values of the weights. In practice, training a Neural Network mostly consists of matrix multiplications. Consider for example _VGG16_ (a frequently used convolutional Neural Network for image classification. Proposed by [K. Simonyan and A. Zisserman](https://arxiv.org/abs/1409.1556) ), it has approximately 140 million parameters. Using a CPU, it would take weeks to train this model and perform all the matrix multiplications.\n",
    "\n",
    "GPUs allow to dramatically decrease the time needed for matrix multiplication, offering 10 to 100 times more computational power than traditional CPUs. There are several reasons why they make this computational speed-up possible, well discussed in [this article](https://themerkle.com/why-gpus-are-ideal-for-deep-learning/).\n",
    "\n",
    "Summarizing the article, GPUs, comprised of thousands of cores unlike CPUs, not only allow for parallel operations, but they are ideal when it comes to fetching enormous amounts of memory. The best GPUs can fetch up to 750GB/s, which is huge if compared with the best CPU which can handle only up to 50GB/s memory bandwidth.\n",
    "Of course, dedicated GPUs, designed explicitly for High-Performance Computing and Deep Learning, are more performant (and expensive) than gaming GPUs, but the latter, usually available in everyday laptops, is still a good starting option!\n",
    "\n",
    "The following picture shows a comparison between CPU and GPU performance (source: [Nvidia](https://www.nvidia.com/en-us/ai-accelerated-analytics/)). The left image shows that the _Fermi GPU_ can process more than ten times the number of images processed (per second) by an _Intel 4 core CPU_. The right image shows that a _16 GPU Accelerated Servers_ can handle a more than six times bigger Neural Network if compared with a _1000 CPU Servers_.\n",
    "\n",
    "![CPU versus GPU](./assets/CPUvsGPU.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cloud GPU providers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As of early 2018, all major cloud providers give access to cloud instances with GPUs. The two leaders in the space are [Amazon Web Services (AWS)](https://aws.amazon.com/) and [Google Cloud Platform (GCP)](https://cloud.google.com/gpu/). These two companies have been pioneers in providing cloud GPUs at affordable rates, and they keep adding new options to their offer. Besides, they both offer additional services specifically built to optimize and serve Deep Learning models at scale.\n",
    "\n",
    "Other companies offering cloud GPUs are [Microsoft Azure Cloud](https://azure.microsoft.com/en-us/)and [IBM](https://www.ibm.com/cloud/gpu). Also, a few startups have started to offer Deep Learning optimized cloud instances, that are often cheaper and easier to access. In this lab we will review [Floydhub](https://www.floydhub.com/), [Pipeline.ai](https://community.cloud.pipeline.ai/admin/app/) and  [Paperspace](https://www.paperspace.com/).\n",
    "\n",
    "Regardless of the cloud provider, if you have a Linux box with an NVIDIA GPU, it is not hard to equip it to run `tensorflow-gpu` and a Jupyter Notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Google Colab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The easiest way to give GPU acceleration a try is to use [Google Colab](https://research.google.com/colaboratory/faq.html), also known as _Colaboratory_. Besides being so easy, Colab is also free to use (you only need a Google account), which makes it perfect to try out GPU acceleration.\n",
    "\n",
    "Colaboratory is a research tool for Machine Learning education and research. It’s a Jupyter Notebook environment that requires no setup to use: you can create and share Jupyter notebooks with others without having to download, install, or run anything on your computer other than a browser. It works with most major browsers, and it is most thoroughly tested with desktop versions of Chrome and Firefox.\n",
    "\n",
    "This [welcome notebook](https://colab.research.google.com/notebooks/welcome.ipynb) provides the information to start working with Colab. In addition to all the standard operations in Jupyter you can change the notebook settings to enable GPU support:\n",
    "![How to activate GPU in Colaboratory](./assets/colab_gpu_setting.png)\n",
    "\n",
    "Once you've done that, you can run this code to verify that GPU is available:\n",
    "\n",
    "```python\n",
    "import tensorflow as tf\n",
    "tf.test.gpu_device_name()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline AI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next best option to try a GPU for free in the cloud is the service offered by [PipelineAI](https://pipeline.ai/). PipelineAI service enables data scientists to train, test, optimize, deploy, and scale models in production rapidly and directly from a Jupyter Notebook or command-line interface. It provides a platform that simplifies the workflow and let the user focus only on the essential Machine Learning aspects.\n",
    "\n",
    "The login process to use PipelineAI is quite simple and straightforward:\n",
    "1. Sign up at [PipelineAI](https://community.cloud.pipeline.ai/admin/app/).\n",
    "2. Once you are successfully logged in, you should see the following dashboard. You can either launch a new notebook or directly type commands in a terminal.\n",
    "![](./assets/Pipeline1.png )<br/><br/>\n",
    "3. Alternatively, you can use some of the already available resources, accessible from the left menu. For example, you can have a look at the `01a_Explore_GPU.ipynb` notebook, under `notebooks > 00_GPU_Workshop`\n",
    "![](./assets/Pipeline2.png )<br/><br/>\n",
    "\n",
    "PipelineAI is not only a platform providing GPU-powered Jupyter Notebooks, but it also allows you to do much more, such as monitoring the training of the algorithms, evaluating the results of your model, comparing the performances of different models, browsing among stored models, and so on. The following picture shows some of the available tools, but have a look at all the options available in the [community edition](https://community.cloud.pipeline.ai/admin/app/).  \n",
    "![](./assets/Pipeline3.png )<br/><br/>\n",
    "\n",
    "To better understand the potential of PipelineAI, we encourage you to take [this tour](https://pipeline.ai/tour/). Pipeline is under active development. You can follow its [Github repository](https://github.com/PipelineAI/pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Floydhub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Floydhub](https://www.floydhub.com/) is an other easy and cheap option to access GPU in the cloud.\n",
    "Floydhub is a platform for training and deploying Deep Learning and AI applications. FloydHub comes with fully configured CPU and GPU environments ready to use for Deep Learning. It includes CUDA, cuDNN and popular frameworks like Tensorflow, PyTorch, and Keras. Please take a look at the [documentation](https://docs.floydhub.com/) for a more extended explanation of its features.\n",
    "\n",
    "[This tutorial](https://docs.floydhub.com/getstarted/quick_start_jupyter/) explains how to start a Jupyter Notebook on Floydhub:\n",
    "1. [Create an account](https://www.floydhub.com/login) on Floydhub.  \n",
    "2. [Install `floyd-cli`](https://docs.floydhub.com/guides/basics/install/) on your computer.\n",
    "```\n",
    "pip install -U floyd-cli\n",
    "```\n",
    "3. [Create a project](https://www.floydhub.com/projects/create), named for example `my_jupyter_project`:\n",
    "![](./assets/flod2.jpg)<br/><br/>\n",
    "\n",
    "4. From your terminal, use `floyd-cli` to initialize the project (be sure to use the name you gave the project in step 3).\n",
    "```\n",
    "floyd init my_jupyter_project\n",
    "```\n",
    "\n",
    ">TIP: if this is the first time you run `floyd` it will ask you to log in. Just type `floyd login` and follow the instructions provided.\n",
    "\n",
    "5. Use again `floyd-cli` to kick off your first Jupyter Notebook.\n",
    "```\n",
    "floyd run --gpu --mode Jupyter\n",
    "```\n",
    "\n",
    "This will confirm the job:\n",
    "\n",
    "![](./assets/floyd_jupyter.png)\n",
    "\n",
    "and open your FloydHub web page. Here you'll see a `View` button that will direct you to a Jupyter Notebook. The notebook is running on FloyHub's GPU servers.\n",
    "\n",
    "![](./assets/floyd_web.png)\n",
    "\n",
    "Once finished you can stop the Jupyter Notebook with the cancel button. Make sure to save your results by downloading the notebook before you terminate it:\n",
    "\n",
    "![](./assets/floyd_terminate.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paperspace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Paperspace](https://www.paperspace.com/) is a platform to access a virtual desktop in the cloud. In particular, the [Gradient](https://www.paperspace.com/gradient) service allows to explore, collaborate, share code and data using Jupyter Notebooks, and submit tasks to the Paperspace GPU cloud.\n",
    "\n",
    "It is a suite of tools specifically designed to accelerate cloud AI and Machine Learning. Gradient also includes a powerful job runner (that can even run on the new Google TPUs!), first-class support for containers and Jupyter notebooks, and a new set of language integrations. Gradient also has a job runner, that allows you to work on your local machine and submit \"jobs\" to the cloud to process. Discover more about this service reading this [blog post](https://blog.paperspace.com/gradient/).\n",
    "\n",
    "The procedure to run a Jupyter Notebooks within Paperspace is similar to what we have seen so far for different GPU services:\n",
    "\n",
    "1. [Create an account](https://www.paperspace.com/account/signup) on Paperspace.\n",
    "2. [Access the console](https://www.paperspace.com/console).\n",
    "![](./assets/paperspace0.png)<br/><br/>\n",
    "3. [Create a Jupyter Notebook](https://www.paperspace.com/console/notebooks) to create your models. (Credit card information on the billing page are required to enable all functionality.).\n",
    "\n",
    "Paperspace is much more general than simply a hosted Jupyter Notebook service with GPU enabled. Since Paperspace gives you a full virtual desktop (both Linux and Windows, as shown in the [following picture](https://blog.paperspace.com/windows-10/)), you can install [any other applications](https://www.paperspace.com/ml) you need, from 3D rendering software to video editing and more.\n",
    "![](./assets/Paperspace1.png)<br/><br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AWS EC2 Deep Learning AMI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AWS provides a [Deep Learning AMI](https://aws.amazon.com/machine-learning/amis/) ready to use with all the NVIDIA drivers pre-installed as well as most Deep Learning frameworks and Python packages. It's not free, but it's sufficiently simple and versatile to use. We can quickly launch Amazon EC2 instances pre-installed with popular Deep Learning frameworks such as Apache MXNet and Gluon, TensorFlow, Microsoft Cognitive Toolkit, Caffe, Caffe2, Theano, Torch, PyTorch, Chainer, and Keras to train sophisticated, custom AI models, experiment with new algorithms, or to learn new skills and techniques.\n",
    "\n",
    "To use any AWS service, we need to [open an account](https://portal.aws.amazon.com/billing/signup?nc2=h_ct). Several resources are available for a free trial period, as described in the [official web page](https://aws.amazon.com/free/). After finishing the trial period, keep in mind that the service will charge you. Also, keep in mind that GPU instances are not included in the free tier so you will incur in charges if you complete the next steps.\n",
    "\n",
    "Follow this procedure to spin up a GPU enabled machine on AWS with the Deep Learning AMI:\n",
    "\n",
    "1. [Access the AWS console](https://console.aws.amazon.com/console/home) and select _EC2_ from the _Compute_ menu.\n",
    "![](./assets/aws1.PNG)<br/><br/>\n",
    "2. Click on the _Launch Instance_ button.\n",
    "![](./assets/aws2.PNG)<br/><br/>\n",
    "3. Scroll the page and select an Amazon Machine Image (AMI). The _Deep Learning AMI_ is a good option to start. It comes in 2 flavors: Ubuntu and Amazon Linux. Both are good, and we recommend you use the flavor you are more comfortable with. Also, note that there are both a _Deep Learning AMI_ and a _Deep Learning AMI Basic_. The _Basic_ AMI has only GPU drivers installed but no Deep Learning software. The full AMI comes pre-packaged with a ton of useful packages including [Tensorflow](https://www.tensorflow.org/), [Keras](https://keras.io/), [Pytorch](https://pytorch.org/), [MXNet](https://mxnet.incubator.apache.org/), [CNTK](https://www.microsoft.com/en-us/cognitive-toolkit/) and more. We recommend you use this one to start.\n",
    "![](./assets/aws3.PNG)<br/><br/>\n",
    "4. Chose an instance type from the menu. Roughly speaking, instance types are ordered in ascending order considering the computational power and the storage space.\n",
    "![](./assets/aws4.PNG)<br/><br/>\n",
    "Here's a summary table of AWS GPU instances. Read the [documentation](https://aws.amazon.com/ec2/instance-types/) for a detailed description of every instance type.\n",
    "![](./assets/aws_gpus.PNG)<br/><br/>\n",
    "\n",
    "\n",
    "Once you have chosen the instance go through the other steps:\n",
    "\n",
    "- Step 3: Configure Instance Details\n",
    "- Step 4: Add Storage\n",
    "- Step 5: Add Tags\n",
    "- Step 6: Configure Security Group: make sure to leave port 22 open for SSH\n",
    "- Step 7: Review Instance Launch\n",
    "\n",
    "and finally, launch your instance with a key pair you own. Let's assume it's called `your-key.pem`.\n",
    "\n",
    "You should now be able to see the newly created instance in the dashboard, and you are now ready to connect with it.\n",
    "![](./assets/aws5.png)<br/><br/>\n",
    "\n",
    "Finally, take a look at the [Tutorials and Examples](https://docs.aws.amazon.com/dlami/latest/devguide/tutorials.html) section to understand better how to use Deep Learning AMI service offered by AWS."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Connect to AMI (Linux)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once your Instance state is `running` you are ready to connect to it. We are going to do that from a terminal. We will use the ssh key we have generated, and we will also route remote port 8888 to the local port 8888 so that we get to access Jupyter Notebook. Go ahead and type:\n",
    "\n",
    "```\n",
    "ssh -i your-key.pem -L 8888:localhost:8888 ubuntu@<your-ip>\n",
    "```\n",
    "\n",
    "> TIP: if you get a message that says your key is not protected, you need to change the permissions of your key to read-only. You can do that by executing the command: `chmod 600 your-key.pem`.\n",
    "\n",
    "Once you're connected you should see a screen like the following, where all the environments are listed:\n",
    "![](./assets/aws_dl_instance.png)\n",
    "\n",
    "We will go ahead and activate the `tensorflow_py36` environment with the command:\n",
    "\n",
    "```\n",
    "source activate tensorflow_p36\n",
    "```\n",
    "\n",
    "and launch Jupyter Notebook with:\n",
    "\n",
    "```\n",
    "nohup jupyter notebook --no-browser &\n",
    "```\n",
    "\n",
    "This command launches Jupyter in a way that will not stop if you disconnect from the instance. The final step is to retrieve the Jupyter address: `http://localhost:8888/?token=<your-token>`. You will find it in the `nohup.out` file:\n",
    "```\n",
    "tail nohup.out\n",
    "```\n",
    "Copy it and paste it into your browser. If you've done everything correctly you should see a screen like this one:\n",
    "![](./assets/jupyter_aws.png)\n",
    "\n",
    "> TIP: Aws also has a tutorial here: https://docs.aws.amazon.com/dlami/latest/devguide/tutorials.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Connect to AMI (Windows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To connect with the AWS EC2 Deep Learning AMI from _Windows_ similar steps must be followed, but in this case it is convenient to use [PuTTY](https://www.chiark.greenend.org.uk/~sgtatham/putty/latest.html), an SSH client specifically developed for the Windows platform.\n",
    "After the installation of Putty in your machine, the procedure to connect with the cloud instance is as follows:\n",
    "\n",
    "1. In the _Session_ palette:\n",
    "    - Host Name (or IP address): `ubuntu@<your-ip>`\n",
    "    - Port: `22`\n",
    "    - Connection type: `SSH`\n",
    "2. In the _Connection > SSH > Auth_ palette:\n",
    "    - Private key file for autentication: browse the key generated by [PuttyGen](https://www.ssh.com/ssh/putty/windows/puttygen)\n",
    "3. In the _Conenction > SSH > Tunnels_ palette:\n",
    "    - Source port: `8888`\n",
    "    - Destination: `localhost:8888`\n",
    "\n",
    "![](./assets/putty0.png)<br/><br/>\n",
    "\n",
    "Once you are connected, follow the same steps as for the Linux case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Turning off the instance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once done with your experiments, remember to turn off the instance to avoid useless costs. Just go to your AWS console and either _Stop_ or _Terminate_ the instance, by choosing an action from the `Actions` menu:\n",
    "\n",
    "![](./assets/stop.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### AWS Command Line Interface"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AWS also supports a command line interface [AWS CLI](https://aws.amazon.com/cli/) that allows performing the same operations from the terminal. If you'd like to try it you can install it using the command:\n",
    "\n",
    "```\n",
    "pip install awscli\n",
    "```\n",
    "\n",
    "from your terminal. Once you have installed it, you need to add configuration credentials. First, you'll have to set up an IAM user in the EC2 dashboard, then run the following configuration command:\n",
    "\n",
    "```\n",
    "aws configure\n",
    "```\n",
    "That will prompt you to insert some information:\n",
    "\n",
    "```\n",
    "AWS Access Key ID [None]: <your access key>\n",
    "AWS Secret Access Key [None]: <your secret>\n",
    "Default region name [None]: us-east-1\n",
    "Default output format [None]: ENTER\n",
    "```\n",
    "\n",
    "[Aws regions and availability zones](https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Concepts.RegionsAndAvailabilityZones.html) are:\n",
    "\n",
    "![](./assets/awsregions.png)\n",
    "\n",
    "Make sure to choose a region that provides a copy of the Deep Learning AMI.\n",
    "\n",
    "As explained in the [AWS CLI guide](https://docs.aws.amazon.com/cli/latest/userguide/cli-chap-configure.html), the output format can be `json`:\n",
    "\n",
    "![](./assets/aws_json.png)\n",
    "\n",
    "or text:\n",
    "![](./assets/aws_text.png)\n",
    "\n",
    "Once configured you can start your Deep Learning instance with the following command:\n",
    "\n",
    "```\n",
    "aws ec2 run-instances \\\n",
    "  --image-id <DL-AMI-ID-for-your-region> \\\n",
    "  --count 1 \\\n",
    "  --instance-type <instance-type> \\\n",
    "  --key-name <your-ssh-key-name> \\\n",
    "  --subnet-id <subnet-id> \\\n",
    "  --security-group-ids <security-group-id> \\\n",
    "  --tag-specifications 'ResourceType=instance,\n",
    "                        Tags=[{Key=Name,Value=<a-name-tag>}]'\n",
    "```\n",
    "\n",
    "Where you will need to insert the following parameters:\n",
    "\n",
    "- `<DL-AMI-ID-for-your-region>`: the AMI ID for the Deep Learning AMI in the AWS region you've chosen\n",
    "- `<instance-type>`: the type of instance, like `g2.2xlarge`, `p3.16xlarge` etc.\n",
    "- `<your-ssh-key-name>`: then give a name to your ssh key. You must have this on your disk.\n",
    "- `<subnet-id>`: the subnet id, you can find this when you launch an instance from the web interface.\n",
    "- `<security-group-id>`: the security group id, you can find this when you launch an instance from the web interface as well.\n",
    "- `<a-name-tag>`: a name for your instance, so that you can easily retrieve it by name\n",
    "\n",
    "You can query the status of your launch with the command:\n",
    "\n",
    "```\n",
    "aws ec2 describe-instances\n",
    "```\n",
    "and remember to stop or terminate the instance when you are done, for example using this command:\n",
    "\n",
    "```\n",
    "aws ec2 terminate-instances --instance-ids <your-instance-id>\n",
    "```\n",
    "which would return something like this:\n",
    "\n",
    "![](./assets/terminate_instance.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AWS Sagemaker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[AWS Sagemaker](https://aws.amazon.com/sagemaker/) is an AWS managed solution that allows performing all the steps involved in a Deep Learning pipeline. In fact, on Sagemaker you can define, train and deploy a Machine Learning model in just a few steps.\n",
    "\n",
    "Sagemaker provides an integrated Jupyter Notebook instance that can be used to access data stored in other AWS services, explore it, clean it and analyze it as well as to define a Machine Learning model. It also provides common Machine Learning algorithms that are optimized to run efficiently against extensive data in a distributed environment.\n",
    "\n",
    "Detailed information about this service is in the official [documentation](https://docs.aws.amazon.com/sagemaker/latest/dg/whatis.html).\n",
    "\n",
    "The procedure to spin up a notebook is similar to what previously seen:\n",
    "\n",
    "1. [Create an AWS account](https://aws.amazon.com/console/?nc1=h_ls) and access the console.\n",
    "2. From the AWS console, select the _Amazon SageMaker_ service, under the Machine Learning group.\n",
    "![](./assets/sage0.PNG)<br/><br/>\n",
    "3. Click the button _Create notebook instance_.\n",
    "![](./assets/sage1.PNG)<br/><br/>\n",
    "4. Assign a _Notebook instance name_, for example, \"my_first_notebook\" and click the button _Create notebook instance_. Sagemaker offers several types of instances, including a cheap option for development of your notebook, a CPU-heavy instance if your code requires a lot of CPUs and a GPU-enabled instance if you need it. Notice that the instance types available for the notebook instance are different from the ones available for model training and deployment.\n",
    "![](./assets/sage2.PNG)<br/><br/>\n",
    "5. Start working on the newly created notebook\n",
    "![](./assets/sage3.png)<br/><br/>\n",
    "\n",
    "Once done developing your model, Sagemaker allows to export, train and deploy the model with straightforward steps. Please refer to the [User guide](https://docs.aws.amazon.com/sagemaker/latest/dg/how-it-works-training.html) for more information on these steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Google Cloud and Microsoft Azure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although we reviewed in detail the solutions offered by Amazon AWS, both Google Cloud and Microsoft Azure offer similarly priced GPU-enabled cloud instances. We invite you to check their offering here:\n",
    "- [Google Cloud]()\n",
    "- [Microsoft Azure]()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The DIY solution (on Ubuntu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you'd like start from scratch on a barebone Linux machine with a GPU, here are the steps you will need to follow:\n",
    "\n",
    "1. Install [NVIDIA Cuda Drivers](https://developer.nvidia.com/cuda-downloads). CUDA is a language that gives direct access to the GPU's virtual instruction set and parallel computational elements, for the execution of compute kernels.\n",
    "- Download and install [CuDNN](https://docs.nvidia.com/deeplearning/sdk/cudnn-install/index.html). CuDNN is an NVIDIA library built on CUDA that implements a lot of common Neural Network algorithms.\n",
    "- Install [Miniconda](https://conda.io/en/latest/miniconda.html). Miniconda is a minimal installation of Python and the `conda` package manager that we will use to install other packages.\n",
    "- Install a few standard packages  `conda install pip numpy pandas scikit-learn scipy matplotlib seaborn h5py`. This command will install the packages in the `base` environment.\n",
    "- Install Tensorflow compiled with GPU support: `pip install tensorflow-gpu`.\n",
    "- (Optional) Install Keras: `pip install keras`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPU VS CPU training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regardless of how you decided to get access to a GPU-enabled cloud instance, in the following code, we will assume that you have access to such an instance and review some functionality that is available in Tensorflow when running on a GPU instance.\n",
    "\n",
    "Let's start by comparing training speed on a CPU vs. a GPU for a Convolutional Neural Network. We will train this on the CIFAR10 data that we have also encountered in [Lab 6](6_Convolutional_Neural_Networks.ipynb). Let's load the usual packages of Numpy, Pandas, and Matplotlib:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('common.py') as fin:\n",
    "    exec(fin.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('matplotlibconf.py') as fin:\n",
    "    exec(fin.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also import Tensorflow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensorflow 2.0 compatibility"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensorflow 2.0 enables [Eager Execution](https://www.tensorflow.org/guide/eager) by default. From our tests this seems to have a problem with the allocation on GPU vs CPU. The issue is documented [here](https://github.com/tensorflow/tensorflow/issues/26244). While the developers at Tensorflow figure out the problem and find a fix, we will disable eager execution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.compat.v1.disable_eager_execution()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolutional model comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we load the data using a helper function that also rescales it and expands the labels to binary categories. If you're unfamiliar with these steps, we recommend you review [Lab 3](3_Machine_Learning.ipynb), [Lab 4](4_Deep_Learning.ipynb) and [Lab 6](6_Convolutional_Neural_Networks.ipynb) where they are repeated multiple times and explained in detail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import cifar10\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading CIFAR10 Data\n"
     ]
    }
   ],
   "source": [
    "def cifar_train_data():\n",
    "    print(\"Loading CIFAR10 Data\")\n",
    "    (X_train, y_train), _ = cifar10.load_data()\n",
    "    X_train = X_train.astype('float32') / 255.0\n",
    "    y_train_cat = to_categorical(y_train, 10)\n",
    "    return X_train, y_train_cat\n",
    "\n",
    "X_conv, y_conv = cifar_train_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we define a function that creates the convolutional model. By now you should be familiar with every line of code that follows, but just as a reminder, we create a `Sequential` model adding layers in sequence, like pancakes in a stack. The layers in this network are:\n",
    "\n",
    "- **2D Convolutional layer** with 32 filters, each of size 3x3 and ReLU activation. Notice that in the first layer we also specify the input shape of `(32, 32, 3)` which means our images are 32x32 pixels with three colors: RGB.\n",
    "- **2D Convolutional layer** with 32 filters, each of size 3x3 and ReLU activation. We add a second convolutional layer immediately after the first to effectively convolve over larger regions in the input image.\n",
    "- **Max Pooling layer** 2 D with a pool size of 2x2. This will cut in half the height and the width of our feature maps, effectively making the calculations four times faster.\n",
    "- **Flatten layer** to go from the order four tensors used by convolutional layers to an order-2 tensor suitable for fully connected networks.\n",
    "- **Fully connected layer** with 512 nodes and a ReLU activation\n",
    "- **Output layer** with ten nodes and a Softmax activation\n",
    "\n",
    "If you need to review these concepts, make sure to check out [Lab 6](6_Convolutional_Neural_Networks.ipynb) for more details.\n",
    "\n",
    "We also compile the model for a classification problem using the **Categorical Cross-entropy** loss function and the **RMSProp** optimizer. These are explained in detail in [Lab 5](5_Deep_Learning_Internals.ipynb).\n",
    "\n",
    "Notice also that we import the `time` module to track the performance of our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Flatten, Dense\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convolutional_model():\n",
    "    print(\"Defining convolutional model\")\n",
    "    t0 = time()\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, (3, 3),\n",
    "                     padding='same',\n",
    "                     input_shape=(32, 32, 3),\n",
    "                     kernel_initializer='normal',\n",
    "                     activation='relu'))\n",
    "    model.add(Conv2D(32, (3, 3), activation='relu',\n",
    "                     kernel_initializer='normal'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(512, activation='relu'))\n",
    "    model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "    print(\"{:0.3f} seconds.\".format(time() - t0))\n",
    "\n",
    "\n",
    "    print(\"Compiling the model...\")\n",
    "    t0 = time()\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer='rmsprop',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    print(\"{:0.3f} seconds.\".format(time() - t0))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to do a comparison between the CPU training time and the GPU training time. We can force tensorflow to create the model on the the CPU with the context setter `with tf.device('cpu:0')`. Let's create a model on the CPU:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defining convolutional model\n",
      "0.089 seconds.\n",
      "Compiling the model...\n",
      "0.111 seconds.\n"
     ]
    }
   ],
   "source": [
    "with tf.device('cpu:0'):\n",
    "    model = convolutional_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 32, 32, 32)        896       \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 30, 30, 32)        9248      \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 15, 15, 32)        0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 7200)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 512)               3686912   \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                5130      \n",
      "=================================================================\n",
      "Total params: 3,702,186\n",
      "Trainable params: 3,702,186\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's train the CPU model for 2 epochs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training convolutional CPU model...\n",
      "Epoch 1/2\n",
      "50000/50000 [==============================] - 99s 2ms/sample - loss: 2.0398 - accuracy: 0.2827\n",
      "Epoch 2/2\n",
      "50000/50000 [==============================] - 104s 2ms/sample - loss: 1.6681 - accuracy: 0.4196\n",
      "203.2117199897766 seconds.\n"
     ]
    }
   ],
   "source": [
    "print(\"Training convolutional CPU model...\")\n",
    "t0 = time()\n",
    "model.fit(X_conv, y_conv,\n",
    "          batch_size=1024,\n",
    "          epochs=2,\n",
    "          shuffle=True)\n",
    "print(\"{:0} seconds.\".format(time() - t0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's compare the model with a model living on the GPU. We use a similar context setter: `with tf.device('gpu:0')`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defining convolutional model\n",
      "0.087 seconds.\n",
      "Compiling the model...\n",
      "0.181 seconds.\n"
     ]
    }
   ],
   "source": [
    "with tf.device('gpu:0'):\n",
    "    model = convolutional_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then we train the model on the GPU:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training convolutional GPU model...\n",
      "Epoch 1/2\n",
      "50000/50000 [==============================] - 4s 75us/sample - loss: 2.0665 - accuracy: 0.2737\n",
      "Epoch 2/2\n",
      "50000/50000 [==============================] - 3s 67us/sample - loss: 1.6770 - accuracy: 0.4141\n",
      "7.567 seconds.\n"
     ]
    }
   ],
   "source": [
    "print(\"Training convolutional GPU model...\")\n",
    "t0 = time()\n",
    "model.fit(X_conv, y_conv,\n",
    "          batch_size=1024,\n",
    "          epochs=2,\n",
    "          shuffle=True)\n",
    "print(\"{:0.3f} seconds.\".format(time() - t0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see training on the GPU is much faster than on the CPU. Also notice that the second epoch runs much faster than the first one. The first epoch also includes the time to transfer the model to the GPU, while for the following ones the model has already been transferred to the GPU. Pretty cool!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NVIDIA-SMI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check that the GPU is actually being utilized using `nvidia-smi`. The [NVIDIA System Management Interface](https://developer.nvidia.com/nvidia-system-management-interface) is a tool that allows us to check the operation of our GPUs. To better understand how it works, have a look at the [documentation](http://developer.download.nvidia.com/compute/DCGM/docs/nvidia-smi-367.38.pdf).\n",
    "\n",
    "To use the NVIDIA System Management Interface:\n",
    "1. Open a new terminal from the Jupyter interface\n",
    "![](./assets/terminal0.PNG)\n",
    "2. Type `nvidia-smi` in the command line."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple GPUs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If your machine has more than one GPU, you can use multiple GPUs to improve your training even more. There are several ways to distribute the training over several GPUs, and the tools to do this are improving and changing very rapidly.\n",
    "\n",
    "We will focus here on the general ideas and suggest a couple of ways to perform parallelization of a model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distribution strategies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many ways to distribute the training across multiple GPUs and even across several machines with many GPUs. Tensorflow has iterated a lot on the API to do this, and the stable version at the time of publication (TF 1.13) offers several distribution strategies through [the `tf.contrib` module](https://www.tensorflow.org/api_docs/python/tf/contrib/distribute). All of these will eventually be ported to TF 2.0, which is the version we are using in this labs.\n",
    "\n",
    "Let's start from the basics.\n",
    "\n",
    "One way to distribute the training across multiple GPUs replicate the same model on each GPU and give each GPU a different batch of data. This is called **data parallelization** or **mirrored strategy**. Using this strategy allows increasing the batch size to N times the original batch size, where N is the number of GPUs available. At each weight update, each GPU receives a different batch of data, runs the forward pass and the back-propagation and then communicates the weigh updates to the CPU, where all the updates are averaged and distributed back to each model on each GPU.\n",
    "\n",
    "With this strategy, the batch size is not limited by the GPU memory. The more GPUs we add, the larger a batch size we can use. Many cloud providers offer instances equipped with eight or even sixteen GPUs, and research groups worldwide published results using hundreds and even thousands of GPUs.\n",
    "\n",
    "The only limitation of this strategy is that the whole model must fit in the GPU memory, so even though the batch size is not capped, the model size is capped.\n",
    "\n",
    "The other way to distribute training across multiple GPUs is to split the model across multiple GPUs, which goes by the name of also called **model parallelization** or **model distribution**. Why would one use this strategy at all? It turns out that many state-of-the-art results, especially those concerning language modeling and natural language understanding, require enormous models, that exceed the capacity of a single GPU. Currently, only researchers and large companies like Google or Amazon use this strategy, but in the future, it will become more accessible and more common also for other users.\n",
    "\n",
    "In the rest of this lab, we will focus on data parallelization.\n",
    "\n",
    "We will introduce it with the most recent API offered by Tensorflow 2.0, but we will also mention a couple of other ways to achieve multi GPU parallelization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Parallelization using Tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensorflow makes it easy to parallelize training by distributing data across multiple GPUs through the `tf.distribute` module. At the time of publishing, although many strategies are available in TF 1.13, TF 2.0 only implements the data parallelization strategy, which we will review here.\n",
    "\n",
    "First, we need to create an instance of the `MirroredStrategy` distribution strategy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "strategy = tf.distribute.MirroredStrategy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we take our model and replicate it across multiple GPUs using the context setter `with`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defining convolutional model\n",
      "0.174 seconds.\n",
      "Compiling the model...\n",
      "0.169 seconds.\n"
     ]
    }
   ],
   "source": [
    "with strategy.scope():\n",
    "    model = convolutional_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point we can train the model normally, but with a larger batch. We define a flag with the number of GPUs (2 in our case):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adjust this to the number of gpus in your machine\n",
    "NGPU = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then we train the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training recurrent GPU model on 2 GPUs ...\n",
      "Epoch 1/2\n",
      "25/25 [==============================] - 4s 154ms/step - loss: 2.1736 - accuracy: 0.2295\n",
      "Epoch 2/2\n",
      "25/25 [==============================] - 2s 67ms/step - loss: 1.8342 - accuracy: 0.3600\n",
      "13.702 seconds.\n"
     ]
    }
   ],
   "source": [
    "print(\"Training recurrent GPU model on {} GPUs ...\".format(NGPU))\n",
    "t0 = time()\n",
    "model.fit(X_conv, y_conv,\n",
    "          batch_size=1024*NGPU,\n",
    "          epochs=2,\n",
    "          shuffle=True)\n",
    "print(\"{:0.3f} seconds.\".format(time() - t0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The API for `tf.distribute` is still in progress and we invite you to check it out periodically to learn about new strategies that get added."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Parallelization using Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keras also has an independent way to parallelize training by distributing data across multiple GPUs. This is achieved through the `multi_gpu_model` command. Let's import it from `keras.utils`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import multi_gpu_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> TIP: if you're on floydhub the keras version is probably earlier than the one we are using in the labs. If you don't find `keras.utils.multi_gpu_model` try with\n",
    "```python\n",
    "from tensorflow.keras.utils.training_utils import multi_gpu_model\n",
    "```\n",
    "or update keras with `pip install --upgrade keras`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's create a new convolutional model (on the cpu):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defining convolutional model\n",
      "0.088 seconds.\n",
      "Compiling the model...\n",
      "0.112 seconds.\n"
     ]
    }
   ],
   "source": [
    "with tf.device(\"/cpu:0\"):\n",
    "    model = convolutional_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and let's distribute it over 2 GPUs (this will only work if you have at least 2 GPUs on your machine):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = multi_gpu_model(model, NGPU, cpu_relocation=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> TIP: you may need to change the `cpu_relocation` parameter to False if your machine has NV-link. Check the [Tensorflow documentation](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/utils/multi_gpu_model) for more information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the model has been parallelized, we need to re-compile it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we can train the model in the exact same way as we did before. Notice that the `multi_gpu_model` documentation explains how a batch is divided to the GPUs:\n",
    "\n",
    "    E.g. if your `batch_size` is 64 and you use `gpus=2`,\n",
    "    then we will divide the input into 2 sub-batches of 32 samples,\n",
    "    process each sub-batch on one GPU, then return the full\n",
    "    batch of 64 processed samples.\n",
    "\n",
    "This also means that if we want to maximize GPU utilization we want to increase the batch size by a factor equal to the number of GPUs, so we will use `batch_size=1024*NGPU`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training recurrent GPU model on 2 GPUs ...\n",
      "Epoch 1/2\n",
      "50000/50000 [==============================] - 4s 83us/sample - loss: 2.1872 - accuracy: 0.2267\n",
      "Epoch 2/2\n",
      "50000/50000 [==============================] - 4s 72us/sample - loss: 1.8637 - accuracy: 0.3500\n",
      "8.523 seconds.\n"
     ]
    }
   ],
   "source": [
    "print(\"Training recurrent GPU model on 2 GPUs ...\")\n",
    "t0 = time()\n",
    "model.fit(X_conv, y_conv,\n",
    "          batch_size=1024*NGPU,\n",
    "          epochs=2,\n",
    "          shuffle=True)\n",
    "print(\"{:0.3f} seconds.\".format(time() - t0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since with 2 GPUs, each epoch takes only a few seconds, let's run the training for a few more epochs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = model.fit(X_conv, y_conv,\n",
    "              batch_size=1024*NGPU,\n",
    "              epochs=30,\n",
    "              shuffle=True,\n",
    "              verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and let's plot the history like we've done many times in this labs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/pdf": "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\n",
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 540x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "pd.DataFrame(h.history).plot()\n",
    "plt.ylim(0, 1.1)\n",
    "plt.axhline(1, color='black');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, with 30 epochs the model seems to be still improving. Having multiple GPUs allowed us to iterate fast and explore the performance of a powerful convolutional model more rapidly. Cool!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Parallelization using Horovod"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Horovod](https://eng.uber.com/horovod/) is an open source framework maintained by Uber that allows easy parallelization of Deep Learning model written in Tensorflow, Keras, PyTorch, and MXNet. Horovod stems from the realization that the High-performance Computing community has been running programs on supercomputers with thousands of CPUs and GPUs for decades now.\n",
    "\n",
    "Instead of designing and implementing an independent parallelization library as Tensorflow did, Horovod took the approach to leverage the existing best practices from the HPC community and use their results to distribute Deep Learning models on multiple devices. In particular, Horovod uses an open implementation of the [Message Passing Interface standard](https://en.wikipedia.org/wiki/Message_Passing_Interface) called [Open MPI](https://www.open-mpi.org/).\n",
    "\n",
    "Horovod is currently not compatible with TF 2.0, so we will refer the interested reader to an [example using Keras and TF 1.13](https://github.com/horovod/horovod/blob/master/examples/keras_mnist.py). There's an [open issue](https://github.com/horovod/horovod/issues/907) tracking this and Horovod's developers are working on it, so stay tuned."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Supercomputing with Tensorflow Mesh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we'd like to mention a new component in the Tensorflow ecosystem called [MESH](https://github.com/tensorflow/mesh). Mesh TensorFlow is aimed ad super computers with many CPUs and GPUs and is not really for the everyday user yet, still it is a very cool project that allows to train incredibly large networks on an arbitrary computing architecture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab we have seen how GPUs can easily be used to train faster on larger data. Before you move on to the next lab make sure to terminate all instances or you'll incur in charges!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In [Exercise 2 of Lab 8](8_NLP_and_Text_Data.ipynb#Exercise-2) we introduced a model for sentiment analysis of the [IMDB](www.imdb.com) dataset provided in Keras. \n",
    "\n",
    "- Reload that dataset and prepare it for training a model:\n",
    "    - choose vocabulary size\n",
    "    - pad the sequences to a fixed length\n",
    "- define a function `recurrent_model(vocab_size, maxlen)` similar to the `convolutional_model` function defined earlier. The function should return a recurrent model.\n",
    "- Train the model on 1 CPU and measure the training time\n",
    "> TIP: This is currently broken. There's an [issue](https://github.com/tensorflow/tensorflow/issues/26245) open about it. The model definition seems to ignore the context setter on the CPU. Just skip this point for now.\n",
    "- Train the model on 1 GPU and measure the training time\n",
    "- Train the model on a machine with more than 1 GPU using `multi_gpu_model` or even better using distribution strategy\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Model parallelism_ is a technique used for models too large to fit in the memory of a single GPU. While this is is not the case for the model we developed in Exercise 1, it is still possible to distribute the model across multiple GPUs using the with context setter. Define a new model with the following architecture:\n",
    "\n",
    "1. Embedding\n",
    "- LSTM\n",
    "- LSTM\n",
    "- LSTM\n",
    "- Dense\n",
    "\n",
    "Place layers 1 and 2 on the first GPU, layers 3 and 4 on the second GPU and the final Dense layer on the CPU.\n",
    "\n",
    "Train the model and see if the performance improves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "title": "Training with GPUs"
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
