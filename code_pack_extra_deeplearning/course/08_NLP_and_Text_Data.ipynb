{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 8: Natural Language Processing and Text Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab, we will learn a few techniques to approach problems involving text. This fundamental topic since textual data is widespread. \n",
    "\n",
    "We will start by introducing text data, and some use cases of Machine Learning and Deep Learning applied to text prediction. Then we will explore the traditional approach to text problems: the **Bag of Words** (BOW) approach. \n",
    "\n",
    "This topic will take us to explore how to extract features from a text. We will introduce new techniques to do this as well as a couple of new Python packages specifically designed to deal with text data.\n",
    "\n",
    "We will explore the limitations of the BOW approach and see how Neural Networks can help to overcome them. In particular, we will look at embeddings to encode text and at how to use them in Keras. Let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use cases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As noted in the introduction text data is encountered in many applications. Let's take a look at a few of them. We are all familiar with **Spam Detection**. It is a text classification problem where we try to distinguish legitimate documents from spammy ones. \n",
    "\n",
    "Spam detection involves email spam, SMS spam, instant messaging spam and in general any corpus of messages. The problem is as a binary classification where one has two sets of documents: the spam messages and the \"ham\" messages, i.e., the legitimate messages that we would like to keep. \n",
    "\n",
    "A similar binary classification problem involving text is that of **Sentiment Analysis**. \n",
    "\n",
    "Imagine you are a rock star tweeting about your latest album. Millions of people will reply to your tweet, and it will be impossible for you to read all of the messages from your fans. You would like to capture the overall sentiment of your fan base and see if they are happy about what you tweeted. \n",
    "\n",
    "Sentiment analysis does that by classifying a piece of text as having *positive* or *negative* overall sentiment. If you know the sentiment for each tweet, it's easy to extract results like: \"74% of your fans responded positively to your tweet\".\n",
    "\n",
    "Many fields use Sentiment Analysis including stock trading, e-commerce reviews, customer service and in general any website or application where users are allowed to submit free-form text comments.\n",
    "\n",
    "![Text problems](./assets/text_problems.png)\n",
    "\n",
    "Extending beyond classification problems, we can consider regression problems involving text, for example extracting a score, a price, or any other metric starting from a text document. An example of this would be estimating the number of followers your tweet will generate based on its text content or predicting the number of downloads your application will do based on the content of a blog article.\n",
    "\n",
    "All the above problems are traditional Machine Learning problems where text is the input to the problem. Text can also be the output of a Machine Learning problem. For example, **Machine Translation** involves converting text from a language to another. It is a supervised, many-to-many, sequence learning problem, where we feed pairs of sentences in two languages to a model that learns to generate the output sequence (for example a sentence in English), given a particular input sequence (the corresponding sentence in Italian).\n",
    "\n",
    "Machine translation is an example of a whole category of Machine Learning problems involving text: problems involving **automatic text generation**. Another famous example in this category is that of **Language Modeling**.\n",
    "\n",
    "In Language Modeling a _corpus of documents_ (see next section for a proper definition) is fed sequentially to a model. The model will learn the probability distribution of a specific word to appear after a sentence. The model is then sampled randomly and is capable of producing sentences that resemble the properties of the corpus. Using this approach people had models produce new sonnets from Shakespeare, [new labs of Harry Potter](https://medium.com/deep-writing/harry-potter-written-by-artificial-intelligence-8a9431803da6), [new episodes of popular novels](https://medium.com/deep-writing/silicon-valley-a-new-episode-written-by-ai-a8f832645bc2) and so on.\n",
    "\n",
    "Since Language Modeling works on sequences, we can also build character level models that learn the syntax or our input corpus. In this way, we can produce syntactically correct markup languages like HTML, Wiki, Latex and even C! See the [wonderful article by Andrej Karpathy](https://karpathy.github.io/2015/05/21/rnn-effectiveness/) for a few examples of this.\n",
    "\n",
    "It is clear that text is involved in several useful application. So let's see how to prepare text documents for Machine Learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading text data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text data is usually a collection of articles or documents. Linguists call this collection a [**corpus**](https://en.wikipedia.org/wiki/Text_corpus) to indicate that it's coherent and organized. For example, we could be dealing with the corpus of patents from our company or with a corpus of articles from a news platform.\n",
    "\n",
    "The first thing we are going to learn is how to load text data using Scikit-Learn. We will build a simple Spam detector to separate SMS containing spam from legitimate SMS messages. The data comes from the [UCI SMS Spam collection](https://archive.ics.uci.edu/ml/datasets/sms+spam+collection), but it is re-organized and compressed.\n",
    "\n",
    "The file `data/sms.zip` is a compressed archive of a folder with the structure:\n",
    "\n",
    "    sms\n",
    "     |-- ham\n",
    "     |    |-- msg_000.txt\n",
    "     |    |-- msg_001.txt\n",
    "     |    |-- msg_003.txt\n",
    "     |    +-- ...\n",
    "     |\n",
    "     +-- spam\n",
    "          |-- msg_002.txt\n",
    "          |-- msg_005.txt\n",
    "          |-- msg_008.txt\n",
    "          +-- ...\n",
    "\n",
    "Let's extract all the data into the data folder:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's import the `zipfile` package from Python so that we can extract the data into folders:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`zipfile` allows to operate directly zipped folder into our workspace. Have a look at the [documentation](https://docs.python.org/2/library/zipfile.html) for further details.  Here we use it to extract the data for later loading it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with zipfile.ZipFile('../data/sms.zip', 'r') as fin:\n",
    "    fin.extractall('../data/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This last operation created a folder called `sms` inside the `data` folder. Let's look at its content. The `os` module contains many functions to interact with the host system. Let's import it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And let's use the command `os.listdir` to look at the content of the folder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ham', 'spam']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir('../data/sms')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected there are two subfolders: `ham` and `spam`. We can count how many files they contain with the help of the following little function that lists the content of `path` and uses a filter to only count files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os.path import isfile, join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_files(path):\n",
    "    files_list = [name for name in os.listdir(path)\n",
    "                  if isfile(join(path, name))]\n",
    "    return len(files_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use this function to count the number of files in the folders:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4825"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ham_count = count_files('../data/sms/ham/')\n",
    "ham_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "747"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spam_count = count_files('../data/sms/spam/')\n",
    "spam_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have 4825 ham files and 747 spam files. We can use these numbers to establish a baseline for our classification efforts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline accuracy: 0.866\n"
     ]
    }
   ],
   "source": [
    "baseline_acc = ham_count / (ham_count + spam_count)\n",
    "print(\"Baseline accuracy: {:0.3f}\".format(baseline_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we always predicted the large class, i.e. we never predicted spam, we would be correct 86.6% of the time. Our model needs to score higher than that to be of any help."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also look at a couple of examples of our messages for each class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(path):\n",
    "    with open(path) as fin:\n",
    "        msg = fin.read()\n",
    "    return msg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "read_file('../data/sms/ham/msg_000.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Ok lar... Joking wif u oni...'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "read_file('../data/sms/ham/msg_001.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&C's apply 08452810075over18's\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "read_file('../data/sms/spam/msg_002.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"FreeMsg Hey there darling it's been 3 week's now and no word back! I'd like some fun you up for it still? Tb ok! XxX std chgs to send, £1.50 to rcv\""
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "read_file('../data/sms/spam/msg_005.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, spam messages look quite different from ham messages. In order to start building a spam detection model, let's first load all the data into a dataset.\n",
    "\n",
    "Scikit Learn offers a function to load text data from folders for classification purposes. Let's use the `load_files` function from `sklearn.datasets` package:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_files('../data/sms/', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`data` is an object of type:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sklearn.utils.Bunch"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The documentation for a `Bunch` reads:\n",
    "\n",
    "    vocabulary-like object, the interesting attributes are: either\n",
    "        data, the raw text data to learn, or 'filenames', the files\n",
    "        holding it, 'target', the classification labels (integer index),\n",
    "        'target_names', the meaning of the labels, and 'DESCR', the full\n",
    "        description of the dataset.\n",
    "\n",
    "so let's look at the available keys:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['data', 'filenames', 'target_names', 'target', 'DESCR'])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following the documentation, let's assign the `data.data` and `data.target` to two variables `docs` and `y`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = data.data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first five text examples in our `docs` variable are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hi Princess! Thank you for the pics. You are very pretty. How are you?',\n",
       " \"Hello my little party animal! I just thought I'd buzz you as you were with your friends ...*grins*... Reminding you were loved and send a naughty adoring kiss\",\n",
       " 'And miss vday the parachute and double coins??? U must not know me very well...',\n",
       " 'Maybe you should find something else to do instead???',\n",
       " 'What year. And how many miles.']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = data.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first five entries in our `y` variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we do anything else, let's save the data we have loaded as a DataFrame, just in case we need to reload it later. As usual we import our common files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('common.py') as fin:\n",
    "    exec(fin.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('matplotlibconf.py') as fin:\n",
    "    exec(fin.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and then create a Dataframe with all the documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>message</th>\n",
       "      <th>spam</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Hi Princess! Thank you for...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Hello my little party anim...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>And miss vday the parachut...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Maybe you should find some...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What year. And how many mi...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         message  spam\n",
       "0  Hi Princess! Thank you for...     0\n",
       "1  Hello my little party anim...     0\n",
       "2  And miss vday the parachut...     0\n",
       "3  Maybe you should find some...     0\n",
       "4  What year. And how many mi...     0"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(docs, columns=['message'])\n",
    "df['spam'] = y\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pandas allows to save a dataframe to a variety of different formats, including Excel, CSV and SAS. We will export to CSV:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('../data/sms_spam.csv',\n",
    "          index=False,\n",
    "          encoding='utf8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature extraction from text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Machine Learning algorithm is not able to deal with text as it is. Instead, we need to extract features from the text!\n",
    "\n",
    "![Feature extraction from text](./assets/text_features.png)\n",
    "\n",
    "Let's begin with a naive solution and gradually build up to a more complex one. The simplest way to build features from a text is to use the counts of certain words that we assume to carry information about the problem.\n",
    "\n",
    "For example, spam messages often offer something for free or give a link to some service. Since these are SMS messages, this link will likely be a number. With these two ideas in mind, let's build a very simple classifier that uses only two features:\n",
    "\n",
    "- The count of the occurrence of the word \"_free_\".\n",
    "- The number of numerical characters.\n",
    "\n",
    "Notice that our text contains uppercase and lowercase words, so as a preprocessing step let's convert everything to lowercase so we don't include meaningless features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_lower = [d.lower() for d in docs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first five entries in our `docs_lower` variable are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hi princess! thank you for the pics. you are very pretty. how are you?',\n",
       " \"hello my little party animal! i just thought i'd buzz you as you were with your friends ...*grins*... reminding you were loved and send a naughty adoring kiss\",\n",
       " 'and miss vday the parachute and double coins??? u must not know me very well...',\n",
       " 'maybe you should find something else to do instead???',\n",
       " 'what year. and how many miles.']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs_lower[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can define a simple helper function that counts the occurrences of a particular word in a sentence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_word(word, sentence):\n",
    "    tokens = sentence.split()\n",
    "    return len([w for w in tokens if w == word])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and apply it to each document:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "free_counts = [count_word('free', d) for d in docs_lower]\n",
    "df = pd.DataFrame(free_counts, columns=['free'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>free</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   free\n",
       "0     0\n",
       "1     0\n",
       "2     0\n",
       "3     0\n",
       "4     0"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly let's build a helper function that counts the numerical character in a sentence using the `re` package:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_numbers(sentence):\n",
    "    return len(re.findall('[0-9]', sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['num_char'] = [count_numbers(d) for d in docs_lower]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>free</th>\n",
       "      <th>num_char</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   free  num_char\n",
       "0     0         0\n",
       "1     0         0\n",
       "2     0         0\n",
       "3     0         0\n",
       "4     0         0"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Spam classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that most messages don't contain our special features, so we don't expect any model to work super well in this case, but let's try to build one anyways. First, let's import the `train_test_split` function from `sklearn` as well as the the usual `Sequential` model and the `Dense` layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's define the helper function that follows the usual process that we repeated several times in the previous labs:\n",
    "\n",
    "- Train/test split\n",
    "- Model definition\n",
    "- Model training\n",
    "- Model evaluation on test set\n",
    "\n",
    "We will use a simple [Logistic Regression model](./03_Machine_Learning.ipynb#Logistic-Regression) to start, to make things simple and quick:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_fit_eval(X, y, model=None,\n",
    "                   epochs=10,\n",
    "                   random_state=0):\n",
    "    X_train, X_test, y_train, y_test = \\\n",
    "    train_test_split(X, y, random_state=random_state)\n",
    "    \n",
    "    if not model:\n",
    "        model = Sequential()\n",
    "        model.add(Dense(1, input_dim=X.shape[1],\n",
    "                        activation='sigmoid'))\n",
    "\n",
    "        model.compile(loss='binary_crossentropy',\n",
    "                      optimizer='adam',\n",
    "                      metrics=['accuracy'])\n",
    "    \n",
    "    h = model.fit(X_train, y_train,\n",
    "                  epochs=epochs,\n",
    "                  verbose=0)\n",
    "    \n",
    "    loss, acc = model.evaluate(X_test, y_test)\n",
    "    \n",
    "    return loss, acc, model, h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Executing the function with our values, we'll capture the result in a variable we'll call `res`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1393/1393 [==============================] - 0s 84us/sample - loss: 0.3055 - accuracy: 0.9713\n"
     ]
    }
   ],
   "source": [
    "res = split_fit_eval(df.values, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the accuracy of our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simple model accuracy: 0.971\n"
     ]
    }
   ],
   "source": [
    "print(\"Simple model accuracy: {:0.3f}\".format(res[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Despite our initial skepticism, this dataset is easy to separate! It is so easy that two simple features  (the counts of the word `free` and the count of numerical characters) already achieve a much better accuracy score than the baseline, that was 0.866."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag of Words features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can extend the simple approach of the previous model in a few ways:\n",
    "\n",
    "- We could build a vocabulary with more than just one word, and build a feature for each of them which counts how many times that word appears.\n",
    "- We could filter out common English words.\n",
    "\n",
    "Scikit Learn has a transformer that does exactly these two tasks, it's called [`CountVectorizer`](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html). Let's import it from `sklearn.feature_extraction.text`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text \\\n",
    "    import CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plan on using the top 3000 most common words in the corpus; this is going to be our **vocabulary size**: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 3000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can initialize the vectorizer. Here we have to use the additional argument `stop_words='english'` that tells the vectorizer to **ignore common English stop words**. We do this because we are ranking features starting from the most common word. If we didn't ignore common words, we would end up with word features like \"if\", \"and\", \"of\" and so on, at the top of our list, since these words are just ubiquitous in the English language. However, these words do not carry much meaning about spam and by ignoring them we get word features that are more specific to our corpus.\n",
    "\n",
    "We are also going to ignore decoding errors using the `decode_error='ignore'` argument:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect = CountVectorizer(decode_error='ignore',\n",
    "                       stop_words='english',\n",
    "                       lowercase=True,\n",
    "                       max_features=vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that it also allows for automatic lowercase conversion. You can check what are the stop words using the `.get_stop_words()` method. Let's look at a few of them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = list(vect.get_stop_words())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['rather',\n",
       " 'every',\n",
       " 'thru',\n",
       " 'next',\n",
       " 'found',\n",
       " 'cannot',\n",
       " 'i',\n",
       " 'themselves',\n",
       " 'side',\n",
       " 'whom']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop_words[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have created the vectorizer, let's apply it to our corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<5572x3000 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 37142 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = vect.fit_transform(docs)\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`X` is a [sparse matrix](https://en.wikipedia.org/wiki/Sparse_matrix) i.e. a matrix in which most of the elements are 0. This makes sense since most messages are short and they will only contain a few of the 3000 words in our feature list. The `X` matrix has 5572 rows (i.e. the total number of sms) and 3000 columns (i.e. the total number of selected words) but only 37142 non-zero entries (less then 1%).\n",
    "\n",
    "To use it for Machine Learning we will convert it to a dense matrix, which we can do by calling `todense()` on the object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xd = X.todense()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> TIP: be careful with converting sparse matrices to dense. If you are dealing with large datasets, you will quickly run out of memory with all those zeros. In those cases, we do on-the-fly conversion to dense of each batch during Stochastic Gradient Descent.\n",
    "\n",
    "Let's also have a look at the features found by the vectorizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = vect.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The features are listed in alphabetical order:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['00',\n",
       " '000',\n",
       " '02',\n",
       " '0207',\n",
       " '02073162414',\n",
       " '03',\n",
       " '04',\n",
       " '05',\n",
       " '06',\n",
       " '07123456789']"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['yogasana', 'yor', 'yr', 'yrs', 'yummy', 'yun', 'yunny', 'yuo', 'yup', 'zed']"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab[-10:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use the helper function we've defined above to train a model on the new features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "maxlines": 10
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1393/1393 [==============================] - 0s 90us/sample - loss: 0.1831 - accuracy: 0.9713\n"
     ]
    }
   ],
   "source": [
    "res = split_fit_eval(Xd, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set accuracy:\t0.971\n"
     ]
    }
   ],
   "source": [
    "print(\"Test set accuracy:\\t{:0.3f}\".format(res[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy on the test set is not much higher than our simple model, however, we can use this model to look for features importances, i.e. to identify words whose weight is high when predicting spam or not. Let's recover the trained model from the `res` object returned by our custom function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = res[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then let's put the weights in a Pandas Series, indexed by the vocabulary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_ = model.get_weights()[0].ravel()\n",
    "vocab_weights = pd.Series(w_, index=vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the top 20 words with positive weights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "txt         0.642381\n",
       "www         0.540770\n",
       "claim       0.531602\n",
       "mobile      0.523435\n",
       "free        0.502965\n",
       "150p        0.492700\n",
       "              ...   \n",
       "stop        0.402874\n",
       "1000        0.396701\n",
       "ringtone    0.389498\n",
       "com         0.380621\n",
       "text        0.380217\n",
       "contact     0.370922\n",
       "Length: 20, dtype: float32"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_weights.sort_values(ascending=False).head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not surprisingly we find here words like `www`, `claim`, `prize`, `cash` etc. Similarly we can look at the bottom 20 words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "lol     -0.448763\n",
       "wat     -0.453553\n",
       "sure    -0.457073\n",
       "did     -0.460798\n",
       "doing   -0.473974\n",
       "yeah    -0.474591\n",
       "           ...   \n",
       "sorry   -0.493651\n",
       "going   -0.527029\n",
       "ll      -0.527689\n",
       "da      -0.542279\n",
       "ok      -0.580417\n",
       "come    -0.587829\n",
       "Length: 20, dtype: float32"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_weights.sort_values(ascending=False).tail(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and see they are pretty common legitimate words like `sorry`, `ok`, `lol`, etc... If we were spammer we could take advantage of this information and craft messages that attempt to fool these simple features by using a lot of words like `sorry`or `ok`. This is a typical **Adversarial Machine Learning** scenario, where the target is constantly trying to beat the model.\n",
    "\n",
    "In any case, it's pretty clear that this dataset is an easy one. So let's load a new dataset and learn a few more tricks!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word frequencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous spam classification problem, we used a `CountVectorizer` transformer from Scikit Learn to produce a sparse matrix with term counts of the top 3000 words. Using the absolute counts was ok with SMS messages because they have a maximum length of 160 characters. In the general case, using absolute counts may be a problem if we deal with documents of uneven length. Think of a brief email versus a long article, both about the topic of AI. The word AI will appear in both documents, but it will likely be repeated more times in the long article. Using the counts would lead us to think that the article is more about AI than the short email, while it's simply a longer text.\n",
    "We can account for that using the **term frequency** instead of the count, i.e., by dividing the counts by the length of the document. Using term frequencies is already an improvement, but we can do even better.\n",
    "\n",
    "There will be some words that are common in every document. These could be [common English stop words](https://en.wikipedia.org/wiki/Stop_words), but they could also be words that are common across the specific corpus. For example, if we are trying to sort a corpus of patents by topics, it is clear that words like _patent_, _application_, _grant_ and similar legal terms will be shared across the whole corpus and not indicative of the particular topic of each of the documents in the corpus.\n",
    "\n",
    "We want to normalize our term frequencies with a term inversely proportional to the fraction of documents containing that term, i.e., we want to use an **inverse document frequency**.\n",
    "\n",
    "These features go by the name of [TF-IDF](https://en.wikipedia.org/wiki/Tf%E2%80%93idf), i.e., **term-frequency–inverse-document-frequency**, which is also available as a vectorizer in Scikit Learn.\n",
    "\n",
    "In other words, TF-IDF features look like:\n",
    "\n",
    "```python\n",
    "TFIDF(word, document) = counts_in_document(word, document) /  counts_of_documents_with_word_in_corpus(word)\n",
    "```\n",
    "\n",
    "or using maths:\n",
    "\\begin{equation}\n",
    "\\text{tf-idf}(w, d)=\\frac{\\text{tf}(w, d)}{\\text{df}(w, d)}\n",
    "\\end{equation}\n",
    "\n",
    "where $w$ is a word, $d$ is a document, $\\text{tf}$ stands for \"term frequency\" and $\\text{df}$ for \"document frequency\".\n",
    "\n",
    "As you can read in the [Wikipedia](https://en.wikipedia.org/wiki/Tf%E2%80%93idf) article, there are several ways to improve the above of the _TF-IDF_ formula, using different regularization schemes. [Scikit Learn implements it as follows](https://scikit-learn.org/stable/modules/feature_extraction.html#text-feature-extraction):\n",
    "\n",
    "\\begin{equation}\n",
    "\\text{tf-idf}(w, d)=\\text{tf}(w, d) \\times \\log{\\left(\\frac{1 + n_d}{1+\\text{df}(w, d)}\\right)} + 1\n",
    "\\end{equation}\n",
    "\n",
    "where $n_d$ is the total number of documents and the regularized logarithm takes care of words that are extremely rare or extremely common."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load a new dataset, containing reviews from the popular website [Rotten Tomatoes](https://www.rottentomatoes.com/):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>review</th>\n",
       "      <th>vote</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Toy story</td>\n",
       "      <td>So ingenious in concept, d...</td>\n",
       "      <td>fresh</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Toy story</td>\n",
       "      <td>The year's most inventive ...</td>\n",
       "      <td>fresh</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Toy story</td>\n",
       "      <td>A winning animated feature...</td>\n",
       "      <td>fresh</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Toy story</td>\n",
       "      <td>The film sports a provocat...</td>\n",
       "      <td>fresh</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Toy story</td>\n",
       "      <td>An entertaining computer-g...</td>\n",
       "      <td>fresh</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       title                         review   vote\n",
       "0  Toy story  So ingenious in concept, d...  fresh\n",
       "1  Toy story  The year's most inventive ...  fresh\n",
       "2  Toy story  A winning animated feature...  fresh\n",
       "3  Toy story  The film sports a provocat...  fresh\n",
       "4  Toy story  An entertaining computer-g...  fresh"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('../data/movie_reviews.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a peek into the data we loaded, see how many reviews we have and a few other pieces of information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 14072 entries, 0 to 14071\n",
      "Data columns (total 3 columns):\n",
      "title     14072 non-null object\n",
      "review    14072 non-null object\n",
      "vote      14072 non-null object\n",
      "dtypes: object(3)\n",
      "memory usage: 329.9+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the division between the `fresh` votes, the `rotten`, and the `none` votes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fresh     0.612067\n",
       "rotten    0.386299\n",
       "none      0.001634\n",
       "Name: vote, dtype: float64"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['vote'].value_counts() / len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the dataset contains reviews about famous movies and a judgment of `rotten` VS `fresh`, which is the class we will try to predict.\n",
    "\n",
    "First of all, we notice that a small number of reviews do not have a class, so let's eliminate those few rows from the dataset. We'll do this by selecting all the `votes` that are _not_ `none`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df.vote != 'none'].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fresh     0.613069\n",
       "rotten    0.386931\n",
       "Name: vote, dtype: float64"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['vote'].value_counts() / len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our reference accuracy is 61.3%, the fraction of the larger class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Label encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the labels are strings, and we need to convert them to 0 and 1 in order to use them for classification. We could do this in many ways, one way is to use the [`LabelEncoder`](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html) from Scikit Learn. It is a transformer that will look at the unique values present in our label column and encode them to numbers from 0 to $N-1$, where $N$ is the number of classes, in our case 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first import it from `sklearn.preprocessing`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's instantiate the `LabelEncoder`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's create a vector of 0s and 1s that represent the features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = le.fit_transform(df['vote'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`y` is now a vector of 0s and 1s. Let's look at the first 10 entries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bag of words: TF-IDF features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's import the `TfidfVectorizer` vector from Scikit Learn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's initialize it to look at the top 10000 words in the corpus, excluding English stop words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 10000\n",
    "\n",
    "vect = TfidfVectorizer(decode_error='ignore',\n",
    "                       stop_words='english',\n",
    "                       max_features=vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the vectorizer to transform our reviews:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = vect.fit_transform(df['review'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<14049x10000 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 130025 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This generates a sparse matrix with 14049 rows and 10000 columns. This is still small enough to be converted to dense and passed to our model evaluation function. Let's call `todense()` on the object to convert it to a dense matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xd = X.todense()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's train our model. We will use a higher number of epochs in this case, to ensure convergence with the larger dataset:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use our function again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "maxlines": 10
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3513/3513 [==============================] - 0s 86us/sample - loss: 0.5133 - accuracy: 0.7504\n"
     ]
    }
   ],
   "source": [
    "res = split_fit_eval(Xd, y, epochs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set accuracy:\t0.750\n"
     ]
    }
   ],
   "source": [
    "print(\"Test set accuracy:\\t{:0.3f}\".format(res[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy on the test set is much lower than the last value of accuracy obtained on the training set (last line printed during training), therefore the model is overfitting. This is not unexpected given the large number of features. Despite the overfitting, the test score is still higher than the 61.3% accuracy obtained by always predicting the larger class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text as a sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The bag of words approach is very crude. It does not take into account context, i.e. each word is treated as independent feature, regardless of its position in the sentence. This is particularly bad for tasks like sentiment analysis where negations could be present (\"This movie was not good\") and the overall sentiment could not be carried by any particular word.\n",
    "\n",
    "To go beyond the bag of words approach, we need to treat the text as a sequence instead of just looking at frequencies. To do this, we will proceed to:\n",
    "\n",
    "1. create a vocabulary, indexed starting from the most frequent word and then continuing in decreasing order.\n",
    "2. convert the sentences into sequences of integer indices using the dictionary\n",
    "3. feed the sequences to a Neural Network to perform the sentiment classification\n",
    "\n",
    "Keras has a preprocessing [`Tokenizer`](https://keras.io/preprocessing/text/) that allows us to create a vocabulary and convert the sentences using it. Let's load it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's initialize the `Tokenizer`. We will use the same vocabulary size of 10000 used in the previous task:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can fit the tokenizer on our reviews using the function `.fit_on_texts`. We will pass the column of the dataframe `df` that contains the reviews:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.fit_on_texts(df['review'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! The tokenizer has finished its job, so let's give a look at some of its attributes.\n",
    "\n",
    "The `.document_count` gives us the number of documents used to build the vocabulary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14049"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.document_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the 14049 reviews left in the dataset after we removed the ones without a vote. The `.num_words` attribute gives us the number of features in the vocabulary. These should be 10000:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.num_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can retrieve the word index by calling `.word_index`, which returns an vocabulary. Let's look at the first 10 items in it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the', 'a', 'and', 'of', 'to', 'is', 'in', 'it', 'that', 'as']"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(tokenizer.word_index)[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see this is not sorted alphabetically, but in decreasing order of frequency starting from the most common word. Let's use the tokenizer to convert our reviews to sequences. For instance, \"The movie is great\" translates to the sequence `1539`:\n",
    "\n",
    "![Conversion of words to indices](./assets/word_to_index.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = tokenizer.texts_to_sequences(df['review'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`sequences` is a list of lists. Each of the inner lists is one of the reviews:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[36,\n",
       "  1764,\n",
       "  7,\n",
       "  1058,\n",
       "  800,\n",
       "  3,\n",
       "  1765,\n",
       "  9,\n",
       "  27,\n",
       "  151,\n",
       "  268,\n",
       "  8,\n",
       "  21,\n",
       "  2,\n",
       "  9088,\n",
       "  3879,\n",
       "  5881,\n",
       "  115,\n",
       "  3,\n",
       "  101,\n",
       "  20,\n",
       "  22,\n",
       "  17,\n",
       "  360],\n",
       " [1, 610, 38, 801, 49],\n",
       " [2, 1012, 347, 225, 9, 24, 107, 14, 564, 21, 1, 354, 7122]]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequences[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's just double check that the conversion is correct by converting the first list back to text. We will need to use the reverse `index -> word` map:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "tok_items = tokenizer.word_index.items()\n",
    "idx_to_word = {i:w for w, i in tok_items}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first review is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'So ingenious in concept, design and execution that you could watch it on a postage stamp-sized screen and still be engulfed by its charm.'"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[0, 'review']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first sequence is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'so ingenious in concept design and execution that you could watch it on a postage stamp sized screen and still be by its charm'"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join([idx_to_word[i] for i in sequences[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The two sentences are almost identical, however notice a couple of things:\n",
    "\n",
    "1. punctuation has been stripped away in the tokenization.\n",
    "- all words are lowercased.\n",
    "- some really rare word (e.g. \"engulfed\") are out of our top 3000 words and are therefore ignored.\n",
    "\n",
    "Now that we have sequences of numbers, we can organize them into a matrix with one review per row and one word per column. Since not all sequences have the same length, we will need to pad the short ones with zeros.\n",
    "\n",
    "Let's calculate the longest sequence length:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "maxlen = max([len(seq) for seq in sequences])\n",
    "maxlen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The longest review contains 49 words. Let's pad every other review to 49 using the [`pad_sequences`](https://keras.io/preprocessing/sequence/) function from Keras:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can read in the documentation:\n",
    "\n",
    "```python\n",
    "Signature: pad_sequences(sequences, maxlen=None, dtype='int32',\n",
    "                         padding='pre', truncating='pre', value=0.0)\n",
    "Docstring:\n",
    "Pads each sequence to the same length (length of the longest sequence).\n",
    "\n",
    "If maxlen is provided, any sequence longer\n",
    "than maxlen is truncated to maxlen.\n",
    "```\n",
    "\n",
    "`pad_sequences` operates on the sequences by padding and truncating them. Let's set the `maxlen` parameter to the value we already found:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pad_sequences(sequences, maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14049, 49)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`X` has 14049 rows (i.e., the number of samples, review in this case) and 49 columns (i.e., the words of the longest review). Let's print out the first few reviews:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,   36, 1764,    7, 1058,  800,    3, 1765,    9,\n",
       "          27,  151,  268,    8,   21,    2, 9088, 3879, 5881,  115,    3,\n",
       "         101,   20,   22,   17,  360],\n",
       "       [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           1,  610,   38,  801,   49],\n",
       "       [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    2, 1012,  347,  225,    9,   24,  107,   14,\n",
       "         564,   21,    1,  354, 7122],\n",
       "       [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    1,\n",
       "          16, 1190,    2,  822,    3,  485,   42,  121,  144,  285,    1,\n",
       "        1678,    4,   13,  742,  724]], dtype=int32)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[:4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can we feed this matrix to a Machine Learning model? Let's think about it for a second. If we treat this matrix as tabular data, it would mean each column represents a feature. What feature? Columns in the matrix correspond to the position of the word in a sentence, and so there's absolutely no reason why two words appearing at the same position would carry consistent information about sentiment.\n",
    "\n",
    "Also, the numbers here are the indices of our words in a vocabulary, so their actual value is not a quantity, it's their rank in order of frequency in the dictionary. In other words, word number 347 is not 347 times as large as the word at index 1; it's just the word that appears at index 347 in the vocabulary.\n",
    "\n",
    "The correct way to think of this data is to recognize that each number in `X` really represents an index in a vector with length `vocab_size`, i.e., a vector with 10000 entries. These are the actual features, i.e., all the words in our vocabulary.\n",
    "\n",
    "So, this matrix is a shorthand for an order-3 sparse tensor whose three axes are (sentence, position along the sentence, word feature index). The first axis would locate the sentence in the dataset, and it corresponds to the row axis of our `X` matrix. The second axis would locate the word position in the sentence, and it corresponds to the column axis of our `X` matrix. The third would be a sparse vector, locate the word in the vocabulary. Instead of using a sparse vector, we use the index of the word in the dictionary, which is the value in that specific entry in the  `X`  matrix.\n",
    "\n",
    "![1-hot encoding of words](./assets/word_to_index_to_1_hot.png)\n",
    "\n",
    "It looks like one way to feed this data to a Neural Network would be to expand the `X` matrix to a 1-hot encoded order-3 tensor with 0s and 1s, and then feed this tensor to our network, for example to a [Recurrent layer](7_Time_Series_and_Recurrent_Neural_Networks.ipynb#Recurrent-Neural-Network) with `input_shape=(49, 10000)`. This would be the equivalent of feeding a dataset of 10000 time series, whose elements are all mostly zeros, except for one at each time, which is not zero when that word occurs in that particular sentence.\n",
    "\n",
    "While this encoding works, it is not really memory efficient. Besides that, representing each word along a different orthogonal axis in a 10000-dimensional vector space doesn't capture any information on how that word is used in its context. How can we improve this situation?\n",
    "\n",
    "One idea would be to insert a fully connected layer to compress our input space from the vast sparse vector space of 10000 words in our vocabulary to a much smaller, dense, vector space, for example with just 32 axes. In this new space, each word is represented by a dense vector, whose entries are floating point numbers instead of all 0s and a single 1.\n",
    "\n",
    "![Dense representation of words with vectors](./assets/word_dense.png)\n",
    "\n",
    "This is cool because now we can feed our sequences of much smaller dense vectors to a recurrent network to complete the sentiment classification task, i.e., we are treating the sentiment classification problem as a [Sequence Classification problem](./07_Time_Series_and_Recurrent_Neural_Networks.ipynb#Sequence-Classification) like the ones encountered in Lab 7.\n",
    "\n",
    "Furthermore, since the dense vector comes from a fully connected layer, we can jointly train the fully connected layer and the recurrent layer allowing the fully connected layer to find the best representation for the words to help the recurrent layer achieve its task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In practice, we never actually go through the burden of converting the word indices to 1-hot vectors and then back to dense vectors. We use an **Embedding layer**. In this layer, we specify the output dimension, i.e., the length of the dense vector, and it has an independent set of as many weights for each of the words in the vocabulary. So, for example, if the vocabulary is 10000 words and we specify an output dim of 100, the embedding layer will carry 1000000 weights, 100 for each of the words in the vocabulary.\n",
    "\n",
    "The numbers in the input indicate the indices that select the set of 100 weights for each word, i.e., they will be interpreted as indices in a phantom sparse space, saving us from converting the data to 1-hot and then converting it back to dense.\n",
    "\n",
    "![Embedding vectors](./assets/word_embedding.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how to include this in our network. Let's load the [`Embedding`](https://keras.io/layers/embeddings/) layers from Keras:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how it works by creating a network with a single such layer that maps a feature space of 100 words to an output dense space of only two dimensions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=100, output_dim=2))\n",
    "model.compile(optimizer='sgd',\n",
    "              loss='categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The network above assumes the inputs are numbers between 0 and 99. We interpret these as the indices of the single non-zero entry in a 100-dimensional 1-hot vector. Sequences of such indices will be interpreted as sequences of such vectors and will be transformed to sequences 2-dimensional dense vectors since 2 is the dimension of the output space.\n",
    "\n",
    "Let's feed a single sequence of a few indices and perform a forward pass:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[-0.03956728,  0.03056333],\n",
       "        [-0.04755666,  0.01909782],\n",
       "        [ 0.04237584,  0.00242125],\n",
       "        [-0.03956728,  0.03056333],\n",
       "        [ 0.01191726,  0.02468473]]], dtype=float32)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(np.array([[ 0, 81,  1, 0, 79]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The embedding layer turned the sequence of five numbers into a sequence of five 2-dimensional vectors. Since we have not trained our Embedding Layer yet, these are just the weight vectors corresponding to each word, so for example, words `0` corresponds to the weights `[-0.03977597, -0.01466479]`. Notice how these appear both on the first row and the fourth row, exactly as one would expect since the words `0` appears at the first and fourth positions in our five words sentence.\n",
    "\n",
    "Similarly, if we feed a batch of few sequences of indices, we will obtain a batch of few sequences of vectors, i.e., a tensor of order three, with axes `(sentence, position in the sentence, embedding)`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[-0.03956728,  0.03056333],\n",
       "        [-0.04755666,  0.01909782],\n",
       "        [ 0.04237584,  0.00242125],\n",
       "        [-0.02041857,  0.0362685 ],\n",
       "        [ 0.01191726,  0.02468473]],\n",
       "\n",
       "       [[-0.03104251, -0.01826168],\n",
       "        [ 0.01193748,  0.01767396],\n",
       "        [-0.04211649, -0.00892004],\n",
       "        [ 0.01063975,  0.03848443],\n",
       "        [-0.02258428,  0.01715442]],\n",
       "\n",
       "       [[-0.01262269,  0.04034625],\n",
       "        [ 0.02745339, -0.01809684],\n",
       "        [-0.01439632, -0.04749357],\n",
       "        [-0.04891055,  0.01495666],\n",
       "        [-0.01743649, -0.01524024]]], dtype=float32)"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(np.array([[ 0, 81,  1, 96, 79],\n",
    "                        [ 4, 17, 47, 69, 50],\n",
    "                        [15, 49,  3, 12, 88]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! Now we know what to do to build our sentiment classifier. We will:\n",
    "\n",
    "- Split as usual our `X` matrix of indices into train and test\n",
    "- Build a network with:\n",
    "    - Embedding\n",
    "    - Recurrent\n",
    "    - Dense\n",
    "- Classify the sentiment of our reviews\n",
    "\n",
    "Let's start from the train/test split. As we've done several times in the labs, we set `random_state=0` so that we all get the same train/test split.\n",
    "\n",
    "> TIP: Setting the random state is useful when you want to have repeatable random splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = \\\n",
    "    train_test_split(X, y, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14049, 49)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Recurrent model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's build our model as we did in the previous lab. First, let's import the `LSTM` layer from Keras:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's build up our model. We'll create our `Embedding` layer followed by our `LSTM` layer and the regular `Dense` and `Activation` layers after that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=vocab_size,\n",
    "                    output_dim=16,\n",
    "                    input_length=maxlen))\n",
    "model.add(LSTM(32))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's train our model using the `fit()` function. We will train the model on batches of 128 reviews for 8 epochs with a 20% validation split:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "maxlines": 8
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8428 samples, validate on 2108 samples\n",
      "Epoch 1/8\n",
      "8428/8428 [==============================] - 2s 210us/sample - loss: 0.6651 - accuracy: 0.6140 - val_loss: 0.6524 - val_accuracy: 0.6067\n",
      "Epoch 2/8\n",
      "8428/8428 [==============================] - 0s 51us/sample - loss: 0.5679 - accuracy: 0.6940 - val_loss: 0.5475 - val_accuracy: 0.7353\n",
      "Epoch 3/8\n",
      "8428/8428 [==============================] - 0s 47us/sample - loss: 0.3983 - accuracy: 0.8379 - val_loss: 0.5708 - val_accuracy: 0.7500\n",
      "Epoch 4/8\n",
      "8428/8428 [==============================] - 0s 46us/sample - loss: 0.2719 - accuracy: 0.8938 - val_loss: 0.6332 - val_accuracy: 0.7519\n",
      "Epoch 5/8\n",
      "8428/8428 [==============================] - 0s 46us/sample - loss: 0.1926 - accuracy: 0.9286 - val_loss: 0.7121 - val_accuracy: 0.7509\n",
      "Epoch 6/8\n",
      "8428/8428 [==============================] - 0s 47us/sample - loss: 0.1380 - accuracy: 0.9528 - val_loss: 0.8569 - val_accuracy: 0.7434\n",
      "Epoch 7/8\n",
      "8428/8428 [==============================] - 0s 46us/sample - loss: 0.1019 - accuracy: 0.9686 - val_loss: 1.0721 - val_accuracy: 0.7415\n",
      "Epoch 8/8\n",
      "8428/8428 [==============================] - 0s 46us/sample - loss: 0.0787 - accuracy: 0.9754 - val_loss: 1.0436 - val_accuracy: 0.7301\n"
     ]
    }
   ],
   "source": [
    "h = model.fit(X_train, y_train, batch_size=128,\n",
    "              epochs=8, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model seems to be doing much better on the training set than any of the previous models based on Bag of Words, since it achieves an accuracy greater than 95% in only 10 epochs. On the other hand, the validation accuracy sees to be consistently lower, which indicates probable overfitting. Let's evaluate the model on the test set in order to verify the ability of our model to generalize:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3513/3513 [==============================] - 0s 73us/sample - loss: 0.9546 - accuracy: 0.7353\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.735269"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss, acc = model.evaluate(X_test, y_test, batch_size=32)\n",
    "acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ouch! The test score is not much better than the score obtained by our BOW model. This means the model is overfitting. Let's plot the training history:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/pdf": "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\n",
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 540x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "dfhistory = pd.DataFrame(h.history)\n",
    "dfhistory[['accuracy', 'val_accuracy']].plot(ylim=(-0.05, 1.05));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, after a few of epochs the validation accuracy stops improving while the training accuracy keeps improving.\n",
    "\n",
    "We can also look at the loss and notice that the validation loss does not decrease after a certain point, while the training loss does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/pdf": "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\n",
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 540x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "dfhistory[['loss', 'val_loss']].plot(ylim=(-0.05, 1.05));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many weights are there in our model? Is it too big?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 49, 16)            160000    \n",
      "_________________________________________________________________\n",
      "unified_lstm (UnifiedLSTM)   (None, 32)                6272      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 166,305\n",
      "Trainable params: 166,305\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is quite big compared to the size of the dataset. We have over 160 thousand parameters to classify less than 15 thousand short reviews. This is not a good situation, and we expect to overfit. In the exercises, we will repeat the sentiment prediction on a larger corpus of reviews and see if we can get better results.\n",
    "\n",
    "We will also learn another way to reduce overfitting later in the labs when we discuss pre-trained models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequence generation and language modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned at the beginning of this lab and in the previous one, Neural Networks are not only suited to deal with textual input data but also to generate text output. We can interpret a text as a **sequence of words**  or as a **sequence of characters** and we can use a model to predict the next character or words in a sequence. This is called **Language Modeling** and it has been successfully used to generate \"Shakespeare-sounding\" poems, new pages of Wikipedia and so on (see [this wonderful article by A. Karpathy](http://karpathy.github.io/2015/05/21/rnn-effectiveness/) for a few examples).\n",
    "\n",
    "The basic idea is to apply to text the same approach we used to [improve forecasting](./07_Time_Series_and_Recurrent_Neural_Networks.ipynb#Improving-Forecasting) in the time series prediction of the last lab.\n",
    "\n",
    "We will start from a corpus of text, split it into short, fixed-size Windows, i.e., sub-sentences with a few characters, and then train a model to predict the next character after the sequence.\n",
    "\n",
    "> What are \"windows\" here? What do they refer to?\n",
    "\n",
    "![Windows of text](./assets/text_to_windows.png)\n",
    "\n",
    "Let's give an example by designing an RNN to generate names of babies. We will use [this corpus](http://www.cs.cmu.edu/afs/cs/project/ai-repository/ai/areas/nlp/corpora/names/) as training data, which contains thousands of names.\n",
    "\n",
    "We start by loading all the names from `../data/names.txt`. We also add a `\\n` character to allow the model to learn to predict the end of a name and convert the names to lowercase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 7939 names\n"
     ]
    }
   ],
   "source": [
    "with open('../data/names.txt') as f:\n",
    "    names = f.readlines()\n",
    "    names = [n.lower().strip() + '\\n' for n in names]\n",
    "\n",
    "print('Loaded %d names' % len(names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at the first three of them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['aamir\\n', 'aaron\\n', 'abbey\\n']"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "names[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to count all of the characters in our \"vocabulary\" and build a vocabulary that translates between the character and its assigned index (and vice versa). We could do this using the `Tokenizer` from Keras, but it is so simple that we can do it by hand using a Python `set`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "chars = set()\n",
    "\n",
    "for name in names:\n",
    "    chars.update(name)\n",
    "\n",
    "vocab_size = len(chars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the number of chars we've saved:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'\\n',\n",
       " '-',\n",
       " 'a',\n",
       " 'b',\n",
       " 'c',\n",
       " 'd',\n",
       " 'e',\n",
       " 'f',\n",
       " 'g',\n",
       " 'h',\n",
       " 'i',\n",
       " 'j',\n",
       " 'k',\n",
       " 'l',\n",
       " 'm',\n",
       " 'n',\n",
       " 'o',\n",
       " 'p',\n",
       " 'q',\n",
       " 'r',\n",
       " 's',\n",
       " 't',\n",
       " 'u',\n",
       " 'v',\n",
       " 'w',\n",
       " 'x',\n",
       " 'y',\n",
       " 'z'}"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's create two dictionaries, one to go fro characters to indices and the other to go back from indices to characters. We'll use these two dictionaries a bit later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_to_idx = dict((c, i) for i, c in enumerate(chars))\n",
    "inds_to_char = dict((i, c) for i, c in enumerate(chars))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Character sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the vocabulary created above to translate each name in `names` to its number format in `int_names`. We will achieve this using a nested [list comprehension](https://docs.python.org/2/tutorial/datastructures.html#list-comprehensions) where we iterate on names and for each name we iterate on characters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "int_names = [[char_to_idx[c] for c in n] for n in names]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now each name has been converted to a sequence of integers, for example, the first name:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'aamir\\n'"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "names[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Was converted to:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[12, 12, 13, 10, 21, 25]"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int_names[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! Now we want to create short sequences of few characters and try to predict the next. We will do this by cutting up names into input sequence of length `maxlen` and using the following character as training labels. Let's start with `maxlen = 3`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxlen = 3\n",
    "\n",
    "name_parts = []\n",
    "next_chars = []\n",
    "\n",
    "for name in int_names:\n",
    "    for i in range(0, len(name) - maxlen):\n",
    "        name_parts.append(name[i: i + maxlen])\n",
    "        next_chars.append(name[i + maxlen])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`name_parts` is a list with short fractions of names (three characters). Let's take a look at the first elements:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[12, 12, 13], [12, 13, 10], [13, 10, 21], [12, 12, 21]]"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name_parts[:4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`next_chars` is a list with single entries, each representing the next character:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[10, 21, 25, 20]"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next_chars[:4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a last step we convert the nested list of `name_parts` to an array. We can do this, using the same `pad_sequences` function used earlier in this lab. This takes the nested list and converts it to an array trimming the longer sequences and padding the shorter sequences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pad_sequences(name_parts, maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final shape of our input is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32016, 3)"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "i.e. we have 32016 name parts, each with 3 consecutive characters.\n",
    "\n",
    "Now let's deal with the labels. We can use the `to_categorical` function to 1-hot encode the targets. Let's import it from `keras.utils`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's create our categories from the `next_chars` using this function. Notice that we let Keras know how many characters are in the vocabulary by setting `num_classes=vocab_size` in the second argument of the function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = to_categorical(next_chars, vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The shape of our labels is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32016, 28)"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "i.e. we have 32016 characters, each represented by a 1-hot encoded vector of `vocab_size` length."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recurrent Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point we are ready to design and train our model. \n",
    "\n",
    "We will need to set up an embedding layer for the input, one or more recurrent layers and a final dense layer with softmax activation to predict the next character. We can design the model using the Sequential API as usual, or we can start to practice with the [Functional API](https://keras.io/getting-started/functional-api-guide/), which we will use more often later on. This API is much more powerful than the [Sequential API](https://keras.io/models/sequential/) we used so far because it allows us to build models that can have more than one processing branch. It is good to start approaching it on a simple case so that we will be more familiar with it when we use it on larger and more complex models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's import the `Model` class from `tensorflow.keras` and the `Input` layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the Functional API, each layer is a function, which receives the output of the previous layer and it returns an output to the next. When we specify a model in this way, we need to start from an `Input` layer with the correct shape.\n",
    "\n",
    "Since we have padded our name subsequences to a length of 3, we'll create an `Input` layer with shape `(3,)`:\n",
    "\n",
    "> TIP: remember that the trailing comma is needed in Python to distinguish a tuple with one element from a simple number within parentheses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = Input(shape=(3, ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the inputs variable we have just defined:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'input_1:0' shape=(None, 3) dtype=float32>"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's a Tensorflow tensor with `shape=(?, 3)`, i.e. it will accept batches of data with 3 features, exactly as we want. Next we create the `Embedding` layer, with input dimension equals to the vocabulary size (i.e. 28) and output dimension equal to 5. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb = Embedding(input_dim=vocab_size, output_dim=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we will use this layer as a function, i.e. we well pass the `inputs` tensor to it and save the output tensor to a temporary variable called `h` (for hidden)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = emb(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we could have achieve the previous two operations in a single line by writing:\n",
    "```python\n",
    "h = Embedding(input_dim=vocab_size, output_dim=5)(inputs)\n",
    "```\n",
    "\n",
    "Following this style, we define the layer to be an `LSTM` layer with eight units, and we reuse the `h` variable for its output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = LSTM(8)(h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we create the output layer, a `Dense` layer with as many nodes as `vocab_size` and with a Softmax activation function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = Dense(vocab_size, activation='softmax')(h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have created all the layers we need and connected their inputs and outputs let's create a model. This is done using the `Model` class that needs to know what the inputs and outputs of the model are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(inputs=inputs, outputs=outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From here onwards we proceed in an identical way to what we've been doing with the Sequential API. We compile the model for a classification problem:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and now we are ready to train it. We will let the training run for at least ten epochs. While the model trains, let us reflect on a couple of questions:\n",
    "\n",
    "- Will this model reach 99% accuracy?\n",
    "- Will any model ever reach 99% accuracy on this task?\n",
    "- Would this change if we had access to a corpus of millions of names?\n",
    "- What accuracy would you expect from randomly guessing the next character?\n",
    "\n",
    "Let's train our model by using the `fit()` function. We will run the training for 20 epochs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "maxlines": 5
   },
   "outputs": [],
   "source": [
    "model.fit(X, y, epochs=20, verbose=0);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! The model has finished training. Getting above 30% accuracy is a good result in this case. The reason is, we are trying to predict the next character after a sequence of three characters, but there is no unique solution to this prediction problem.\n",
    "\n",
    "Think for example of the three characters `and`. How many names are there in the dataset that start with `and`?\n",
    "\n",
    "- `anders` -> next char is `e`\n",
    "- `andie` -> next char is `i`\n",
    "- `andonis` -> next char is `o`\n",
    "- `andre` -> next char is `r`\n",
    "- `andrea` -> next char is `r`\n",
    "- `andreas` -> next char is `r`\n",
    "- `andrej` -> next char is `r`\n",
    "- `andres` -> next char is `r`\n",
    "- `andrew` -> next char is `r`\n",
    "- `andrey` -> next char is `r`\n",
    "- `andri` -> next char is `r`\n",
    "- `andros` -> next char is `r`\n",
    "- `andrus` -> next char is `r`\n",
    "- `andrzej` -> next char is `r`\n",
    "- `andy` -> next char is `y`\n",
    "\n",
    "From this example, we see that while `r` is the most frequent answer, it's not the only one. Other letters could come after the letters `and` in our training set.\n",
    "\n",
    "By training the model on the truncated sequences, we are effectively teaching our model a probability distribution over our vocabulary. Using the example above, given the series of characters `['a', 'n', 'd']` the model is learning that the character `r` appears 11/15 times, i.e., it has a probability of 0.733, while the characters `e`, `i`, `o`, `y` each appear 1/15 times, i.e., each has a probability of 0.066.\n",
    "\n",
    "> TIP: For the math inclined reader, the model is learning to predict the probability  $p(c_t | c_{t-3}c_{t-2}c_{t-1})$ where the index $t$ indicates the position of a character in the name. $p(A | B)$ is the **conditional probability** of A given B. This is the probability that A will happen when B has already happened.\n",
    "\n",
    "Since the vocabulary size is 28, if the next character would have been predicted using a random uniform distribution over the vocabulary, on average we would predict correctly only one time every 28 trials, which would give an accuracy of about 3.6%. We get to an accuracy of about 30%, which is 10x higher than random."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sampling from the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we trained the model, we can use it to produce new names, that should at least sound like English names. We can sample the model by feeding in a few letters and using the model's prediction for the next letter. Then we feed the model's prediction back in to get the next letter, and so on.\n",
    "\n",
    "First of all, let's define a helper function called `sample`. This function has to take an array of probabilities $\\mathbf{p} = [p_i]_{i \\in \\textrm{vocab}}$ for the characters in the vocabulary and return the index of a character, with probabilities according to $\\mathbf{p}$. This will return more probable characters more often than characters with a lower probability.\n",
    "\n",
    "The [**multinomial distribution**](https://en.wikipedia.org/wiki/Multinomial_distribution) is a generalization of the binomial distribution that can help us in this case. It is implemented in Numpy and its [documentation](https://docs.scipy.org/doc/numpy/reference/generated/numpy.random.multinomial.html) reads:\n",
    "\n",
    "    The multinomial distribution is a multivariate generalization of the\n",
    "    binomial distribution.  Take an experiment with one of ``p``\n",
    "    possible outcomes.  An example of such an experiment is throwing a dice,\n",
    "    where the outcome can be 1 through 6.  Each sample drawn from the\n",
    "    distribution represents `n` such experiments.  Its values,\n",
    "    ``X_i = [X_0, X_1, ..., X_p]``, represent the number of times the\n",
    "    outcome was ``i``.\n",
    "\n",
    "\n",
    "This description says that if our experiment has three possible outcomes with probabilities `[0.25, 0.7, 0.05]`, a single multinomial draw will return an array of length three, where all the entries will be zero except one, that will be a 1, corresponding to the randomly chosen outcome for that experiment. If we were to repeat the draws multiple times, the frequencies of each outcome would tend towards the assigned probabilities.\n",
    "\n",
    "Therefore we can implement the sample function as:\n",
    "\n",
    "\\begin{equation}\n",
    "\\textrm{sample}(p) := \\textrm{argmax}(\\textrm{multinomial}(1, p, 1))\n",
    "\\end{equation}\n",
    "\n",
    "We are going to generalize this a bit more, introducing a parameter called **diversity** that rescales the probabilities. For high values of the diversity, the probability vectors will tend to zero, and we will be approaching the random uniform distribution. When the diversity is low, the most likely characters will be selected even more often, approaching a deterministic character generator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create the `sample` function that accepts an input list with a `diversity` argument that allows us to rescale the probabilities as an argument:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(p, diversity=1.0):\n",
    "    p1 = np.asarray(p).astype('float64')\n",
    "    p1 = np.log(p1) / diversity\n",
    "    e_p1 = np.exp(p1)\n",
    "    s = np.sum(e_p1)\n",
    "    p1 = e_p1 / s\n",
    "    return np.argmax(np.random.multinomial(1, p1, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make sure we understand how this function works with an example. Let's define the probabilities of 3 outcomes (you may think of these as win-loose-draw) where the first one happens 1/4th of the time, the second one 65% of the time and the last one only 10% of the time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs = [0.25, 0.65, 0.1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drawing samples from this probability distribution we would expect to pull out the number 1 (corresponding to the second outcome) about 65% of the time and so on.\n",
    "Let's sample 100 times:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "draws = [sample(probs) for i in range(100)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and let's use a `Counter` to count how many of each we drew:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({1: 65, 2: 14, 0: 21})"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(draws)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see our results reflect the actual probabilities, with some statistical fluctuations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! Now that we can sample from the vocabulary let's generate a few names. We will start from an input *seed* of three letters and then iterate in a loop the following steps:\n",
    "\n",
    "- Use the seed to predict the probability distribution for next characters.\n",
    "- Sample the distribution using the sample function.\n",
    "- Append the next character to the seed.\n",
    "- Shift the input window by one to include the last character appended.\n",
    "- Repeat.\n",
    "\n",
    "The loop ends either when we reach a termination character or a pre-defined length.\n",
    "\n",
    "Let's go ahead and build this function step by step. Let's set up the `seed` of our name to be something like `ali`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 'ali'\n",
    "out = seed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to build the name, let's create an output list (we'll call `x`) to store our output, setting the length to that of the maximum length of the name we want to generate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.zeros((1, maxlen), dtype=int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use a variable we'll call `stop` to stop the loop if our network predicts the `'\\n'` character as the next character and set it to `False`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally let's loop until we have to stop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'alistus\\n'"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "while not stop:\n",
    "    for i, c in enumerate(out[-maxlen:]):\n",
    "        x[0, i] = char_to_idx[c]\n",
    "\n",
    "    preds = model.predict(x, verbose=0)[0]\n",
    "\n",
    "    c = inds_to_char[sample(preds)]\n",
    "    out += c\n",
    "\n",
    "    if c == '\\n':\n",
    "        stop = True\n",
    "out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The network produced a few characters and then stopped. Now let's wrap these steps in a function that encapsulates this entire process in a single method. Let's call our function `complete_name`. This function will take an input *seed* of three letters and run through the previous steps to predict the next character."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "def complete_name(seed, maxlen=3, max_name_len=None,\n",
    "                  diversity=1.0):\n",
    "    '''\n",
    "    Completes a name until a termination character is\n",
    "    predicted or max_name_len is reached.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    seed : string\n",
    "        The start of the name to sample\n",
    "    maxlen : int, default 3\n",
    "        The size of the model's input\n",
    "    max_name_len : int, default None\n",
    "        The maximum name length; if None then samples\n",
    "        are generated until the model generates a '.'\n",
    "    diversity : float, default 1.0\n",
    "        Parameter to increase or decrease the randomness\n",
    "        of the samples; higher = more random,\n",
    "        lower = more deterministic\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    out : string\n",
    "    '''\n",
    "    \n",
    "    out = seed\n",
    "    \n",
    "    x = np.zeros((1, maxlen), dtype=int)\n",
    "    \n",
    "    stop = False\n",
    "    \n",
    "    while not stop:\n",
    "        for i, c in enumerate(out[-maxlen:]):\n",
    "            x[0, i] = char_to_idx[c]\n",
    "        \n",
    "        preds = model.predict(x, verbose=0)[0]\n",
    "        \n",
    "        c = inds_to_char[sample(preds, diversity)]\n",
    "        out += c\n",
    "        \n",
    "        if c == '\\n':\n",
    "            stop = True\n",
    "        else:\n",
    "            if max_name_len is not None:\n",
    "                if len(out) > max_name_len - 1:\n",
    "                    out = out + '\\n'\n",
    "                    stop = True\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice! Now that we have a function to complete names, let's predict a few names that start as `jen`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jenlie\n",
      "\n",
      "jenelorla\n",
      "\n",
      "jenda\n",
      "\n",
      "jen\n",
      "\n",
      "jenela\n",
      "\n",
      "jenny\n",
      "\n",
      "jen\n",
      "\n",
      "jenryme\n",
      "\n",
      "jenansa\n",
      "\n",
      "jendee\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print(complete_name('jen'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not bad! Let's play with the _diversity_ parameter to understand what it does. If we set the diversity to be high, we get random sequences of characters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jensilregtpliodmzyti\n",
      "\n",
      "jenq\n",
      "\n",
      "jennortvariiejb-pawf\n",
      "\n",
      "jensllaxfpqs\n",
      "\n",
      "jenedbtvxlleieadxold\n",
      "\n",
      "jen\n",
      "\n",
      "jenfiiquxegvdqj-s-in\n",
      "\n",
      "jencgyvopyrdceg\n",
      "\n",
      "jene\n",
      "\n",
      "jenllectcubvfdcifdnr\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print(complete_name('jen', diversity=10,\n",
    "                        max_name_len=20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we set it to a small value, the function becomes deterministic.\n",
    "\n",
    "> TIP: since the `sample` function involves logarithms and exponential, it accumulates numerical errors very quickly. It would be better to build a model that predicts logits instead of probabilities, but Keras does not allow to do that. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jen\n",
      "\n",
      "jen\n",
      "\n",
      "jen\n",
      "\n",
      "jen\n",
      "\n",
      "jen\n",
      "\n",
      "jen\n",
      "\n",
      "jen\n",
      "\n",
      "jen\n",
      "\n",
      "jen\n",
      "\n",
      "jen\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print(complete_name('jen', diversity=0.01, \n",
    "                        max_name_len=20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Awesome! We now know how to build a language model! Go ahead and unleash your powers on your author of choice and start producing new poems or stories. The model we built has a memory of 3 characters, so it won't exactly be \"Shakespeare\" when it tries to produce sentences. To have a model producing correct sentences in English, we would need to train it with a much larger corpus and with longer Windows of text. For example, a memory of 20-25 characters is long enough to generate English-looking text.\n",
    "\n",
    "In the next section, we will extend our skills to build a language translation model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sequence to sequence models and language translation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sequence-to-sequence (Seq2Seq)**  models take a sentence in input and return a new sequence in output. They are very common in language translation, where the input sequence is a sentence in the first language, and the output sequence is the translation in the second language.\n",
    "\n",
    "![Encoder - Decoder network](./assets/encoder_decoder.png)\n",
    "\n",
    "There is a [great article by Francois Chollet on the Keras Blog](https://blog.keras.io/a-ten-minute-introduction-to-sequence-to-sequence-learning-in-keras.html) on how to build them in Keras. We strongly encourage you to read it!\n",
    "\n",
    "In this lab, we have approached text problems from a variety of angles and hopefully inspired you to dig deeper into this domain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our Spam detection model, we used a `CountVectorizer` with a vocabulary size of 3000. Was this the best size? Let's find out:\n",
    "\n",
    "- reload the spam dataset\n",
    "- do a train test split with `random_state=0` on the SMS data frame\n",
    "- write a function `train_for_vocab_size` that takes `vocab_size` as input and does the following:\n",
    "    - initialize a `CountVectorizer` with `max_features=vocab_size`\n",
    "    - fit the vectorizer on the training messages\n",
    "    - transform both the training and the test messages to count matrices\n",
    "    - train the model on the training set\n",
    "    - return the model accuracy on the training and test set\n",
    "- plot the behavior of the train and test set accuracies as a function of `vocab_size` for a range of different vocab sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keras provides a large dataset of movie reviews extracted from the [Internet Movie Database](www.imdb.com) for sentiment analysis purposes. This dataset is much larger than the one we have used, and its already encoded as sequences of integers. Let's put what we have learned to good use and build a sentiment classifier for movie reviews:\n",
    "\n",
    "- decide what size of vocabulary you are going to use and set the `vocab_size` variable\n",
    "- import the `imdb` module from `keras.datasets`\n",
    "- load the train and test sets using `num_words=vocab_size`\n",
    "- check the data you have just loaded; they should be sequences of integers\n",
    "- pad the sequences to a fixed length of your choice. You will need to:\n",
    "    - decide what a reasonable length to express a movie review is\n",
    "    - decide if you are going to truncate the beginning or the end of reviews that are longer than such length\n",
    "    - decide if you are going to pad with zeros at the beginning or the end for reviews that are shorter than such length\n",
    "- build a model to do sentiment analysis on the truncated sequences\n",
    "- train the model on the training set\n",
    "- evaluate the performance of the model on the test set\n",
    "\n",
    "Bonus points: can you convert back the sentences to their original text form? You should look at `imdb.get_word_index()` to download the word index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "title": "Natural Language Processing and Text Data"
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
